{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS505FinalProject_final_version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7wB6kn_p5g"
      },
      "source": [
        "# By Mingcheng Xu\n",
        "# This notebook contains 5 models:\n",
        "\n",
        "1. Linear regression with word length, word frequency and pre-trained Glove embeddings\n",
        "\n",
        "2. LSTM classification\n",
        "\n",
        "3. LSTM classification with Glove\n",
        "\n",
        "4. LSTM that predicts complexity as a float number\n",
        "\n",
        "5. LSTM that predicts a float number, with Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK4EY3lXrXbB",
        "outputId": "8535c39e-801d-419f-df5b-6eb570c9019c"
      },
      "source": [
        "!pip install syllables"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syllables in /usr/local/lib/python3.7/dist-packages (0.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK0gMeXPKEEX"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "import torchtext.vocab\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import syllables"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYtWoHTOKcNk",
        "outputId": "964b61ec-ef2e-455a-c3df-df4b839e5107"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNkKxGDiKcIN"
      },
      "source": [
        "# the paths; change when necessary\n",
        "TRAIN_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train.tsv\"\n",
        "TEST_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test.tsv\"\n",
        "TRAIN = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train_cleaned.tsv\"\n",
        "TEST = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test_cleaned.tsv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVQ9mky8KcFb"
      },
      "source": [
        "# read the datasets\n",
        "# train\n",
        "with open(TRAIN_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# need to remove \" from the string, otherwise parsing will have problems because some quotas are not closed \n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TRAIN, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "df = pd.read_csv(TRAIN, sep='\\t')\n",
        "\n",
        "# test\n",
        "with open(TEST_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TEST, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "test = pd.read_csv(TEST, sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "Vw8Gk02zKcCc",
        "outputId": "a5b0b72d-feac-4dd6-f205-0a059ad2a9ff"
      },
      "source": [
        "# take a look\n",
        "pd.set_option('display.max_colwidth', None) # show the whole sentence\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3ZLW647WALVGE8EBR50EGUBPU4P32A</td>\n",
              "      <td>bible</td>\n",
              "      <td>Behold, there came up out of the river seven cattle, sleek and fat, and they fed in the marsh grass.</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34R0BODSP1ZBN3DVY8J8XSIY551E5C</td>\n",
              "      <td>bible</td>\n",
              "      <td>I am a fellow bondservant with you and with your brothers, the prophets, and with those who keep the words of this book.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</td>\n",
              "      <td>bible</td>\n",
              "      <td>The man, the lord of the land, said to us, 'By this I will know that you are honest men: leave one of your brothers with me, and take grain for the famine of your houses, and go your way.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFNCI9LYKQN09BHXHH9CLSX5KP738</td>\n",
              "      <td>bible</td>\n",
              "      <td>Shimei had sixteen sons and six daughters; but his brothers didn't have many children, neither did all their family multiply like the children of Judah.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</td>\n",
              "      <td>bible</td>\n",
              "      <td>He has put my brothers far from me.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3ZLW647WALVGE8EBR50EGUBPU4P32A  bible  ...     river   0.000000\n",
              "1  34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible  ...  brothers   0.000000\n",
              "2  3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible  ...  brothers   0.050000\n",
              "3  3BFNCI9LYKQN09BHXHH9CLSX5KP738  bible  ...  brothers   0.150000\n",
              "4  3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  bible  ...  brothers   0.263889\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "UTLwl2AWa-XG",
        "outputId": "837b5bfa-9d09-4aec-fe39-4be9ab87aa56"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3K8CQCU3KE19US5SN890DFPK3SANWR</td>\n",
              "      <td>bible</td>\n",
              "      <td>But he, beckoning to them with his hand to be silent, declared to them how the Lord had brought him out of the prison.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3Q2T3FD0ON86LCI41NJYV3PN0BW3MV</td>\n",
              "      <td>bible</td>\n",
              "      <td>If I forget you, Jerusalem, let my right hand forget its skill.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B</td>\n",
              "      <td>bible</td>\n",
              "      <td>the ten sons of Haman the son of Hammedatha, the Jew's enemy, but they didn't lay their hand on the plunder.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD</td>\n",
              "      <td>bible</td>\n",
              "      <td>Let your hand be lifted up above your adversaries, and let all of your enemies be cut off.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3QREJ3J433XSBS8QMHAICCR0BQ1LKR</td>\n",
              "      <td>bible</td>\n",
              "      <td>Abimelech chased him, and he fled before him, and many fell wounded, even to the entrance of the gate.</td>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3K8CQCU3KE19US5SN890DFPK3SANWR  bible  ...      hand   0.000000\n",
              "1  3Q2T3FD0ON86LCI41NJYV3PN0BW3MV  bible  ...      hand   0.197368\n",
              "2  3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B  bible  ...      hand   0.200000\n",
              "3  3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD  bible  ...      hand   0.267857\n",
              "4  3QREJ3J433XSBS8QMHAICCR0BQ1LKR  bible  ...  entrance   0.000000\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "NsykAZ1Zi6ti",
        "outputId": "44ddf643-e4ec-4689-dcc7-2bfe80ce5197"
      },
      "source": [
        "# the distribution of the complexities in training set\n",
        "look_complexities = df['complexity'].to_list()\n",
        "x_axis = [i for i in range(len(look_complexities))]\n",
        "plt.scatter(x_axis, look_complexities)\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3ToWiZQlyk"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPdQRWvvaQyX"
      },
      "source": [
        "#Try linear regression first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYqivG31Lubb"
      },
      "source": [
        "def create_weights_matrix(vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in Glove \"\"\"\n",
        "  matrix_len = len(vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension))\n",
        "\n",
        "  for i, word in enumerate(vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = glove[word]\n",
        "      except KeyError:\n",
        "          weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, )) # initialize a random vector\n",
        "  #return torch.from_numpy(weights_matrix) # a tensor\n",
        "  return weights_matrix"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOsDaEgacAF"
      },
      "source": [
        "# use the Glove 6B 100d\n",
        "cache_dir = \"/content/gdrive/My Drive/Colab Notebooks/data\"\n",
        "# glove = vocab.pretrained_aliases[\"glove.6B.100d\"](cache=cache_dir)\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100, cache=cache_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCVuZWyZEXm",
        "outputId": "75e499cb-c96a-4429-8e3c-69177019deb5"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x_EbtYiab9v",
        "outputId": "ea29fb8b-5cea-41f3-b4f1-7d51520faa2d"
      },
      "source": [
        "# get all the non-unique tokens for prediction\n",
        "tokens = df['token'].dropna().to_list() # nan should be in the last row which is empty\n",
        "tokens = [token.lower() for token in tokens] # lowercase\n",
        "print(len(tokens))\n",
        "\n",
        "# check if all tokens are in Glove\n",
        "for token in tokens:\n",
        "  if token not in glove.stoi:\n",
        "    print(\"Token Not Found: \")\n",
        "    print(token)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7659\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "housetops\n",
            "Token Not Found: \n",
            "slanderers\n",
            "Token Not Found: \n",
            "plowmen\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dunghill\n",
            "Token Not Found: \n",
            "carotids\n",
            "Token Not Found: \n",
            "tace\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "aaZuP5QmeGgf",
        "outputId": "aace90ca-a382-4087-c6a3-e1fb3bf1f988"
      },
      "source": [
        "# create a dataframe for linear regression\n",
        "train_df = pd.DataFrame(tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "train_df['complexity'] = df['complexity']\n",
        "\n",
        "# word length\n",
        "train_df['word_length'] = train_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# punctuations\n",
        "punc = string.punctuation\n",
        "\n",
        "# stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = df['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count_word_frequency = Counter(texts)\n",
        "train_df['word_frequency'] = train_df['token'].map(lambda x: count_word_frequency[x])\n",
        "\n",
        "# syllables\n",
        "train_df['syllable'] = train_df['token'].map(lambda x: syllables.estimate(x))\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>syllable</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  word_frequency  syllable\n",
              "0     river    0.000000            5              26         2\n",
              "1  brothers    0.000000            8              36         2\n",
              "2  brothers    0.050000            8              36         2\n",
              "3  brothers    0.150000            8              36         2\n",
              "4  brothers    0.263889            8              36         2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "KU-Kz_ThLuYD",
        "outputId": "a098200a-ae08-4912-ce38-5cbae28c054e"
      },
      "source": [
        "# create the weight matrix\n",
        "weights_matrix = create_weights_matrix(tokens)\n",
        "print(weights_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weights_matrix_df = pd.DataFrame(weights_matrix)\n",
        "\n",
        "train_df_combined = pd.concat([train_df, weights_matrix_df], axis=1)\n",
        "train_df_combined.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7659, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>syllable</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.33249</td>\n",
              "      <td>-0.56631</td>\n",
              "      <td>0.54255</td>\n",
              "      <td>-0.11869</td>\n",
              "      <td>0.531290</td>\n",
              "      <td>-0.49381</td>\n",
              "      <td>0.64114</td>\n",
              "      <td>0.85982</td>\n",
              "      <td>0.39633</td>\n",
              "      <td>-1.53950</td>\n",
              "      <td>-0.30613</td>\n",
              "      <td>0.97267</td>\n",
              "      <td>-0.31192</td>\n",
              "      <td>-0.10311</td>\n",
              "      <td>0.359510</td>\n",
              "      <td>-0.60023</td>\n",
              "      <td>0.909830</td>\n",
              "      <td>-0.959540</td>\n",
              "      <td>-0.55375</td>\n",
              "      <td>0.082818</td>\n",
              "      <td>0.26711</td>\n",
              "      <td>0.64645</td>\n",
              "      <td>-0.098556</td>\n",
              "      <td>0.539240</td>\n",
              "      <td>-0.21810</td>\n",
              "      <td>-0.13430</td>\n",
              "      <td>-1.80700</td>\n",
              "      <td>-0.14879</td>\n",
              "      <td>0.39006</td>\n",
              "      <td>-0.62883</td>\n",
              "      <td>-0.38825</td>\n",
              "      <td>0.31925</td>\n",
              "      <td>0.77853</td>\n",
              "      <td>-0.60273</td>\n",
              "      <td>0.063585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.53185</td>\n",
              "      <td>0.72585</td>\n",
              "      <td>0.36811</td>\n",
              "      <td>0.19494</td>\n",
              "      <td>0.64276</td>\n",
              "      <td>0.81460</td>\n",
              "      <td>0.26748</td>\n",
              "      <td>-0.39275</td>\n",
              "      <td>0.425950</td>\n",
              "      <td>0.11699</td>\n",
              "      <td>0.21063</td>\n",
              "      <td>-0.061747</td>\n",
              "      <td>0.79298</td>\n",
              "      <td>-0.45978</td>\n",
              "      <td>0.85176</td>\n",
              "      <td>-0.36726</td>\n",
              "      <td>0.11816</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>-0.065352</td>\n",
              "      <td>0.69672</td>\n",
              "      <td>0.37525</td>\n",
              "      <td>0.92586</td>\n",
              "      <td>-0.83036</td>\n",
              "      <td>-0.087948</td>\n",
              "      <td>-0.49715</td>\n",
              "      <td>0.21411</td>\n",
              "      <td>-0.82838</td>\n",
              "      <td>-0.85912</td>\n",
              "      <td>0.61576</td>\n",
              "      <td>1.18800</td>\n",
              "      <td>-0.30745</td>\n",
              "      <td>-1.20090</td>\n",
              "      <td>-1.70970</td>\n",
              "      <td>0.51400</td>\n",
              "      <td>-1.01590</td>\n",
              "      <td>0.55555</td>\n",
              "      <td>-1.03850</td>\n",
              "      <td>-0.69940</td>\n",
              "      <td>1.050600</td>\n",
              "      <td>0.24051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  105 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...       97        98       99\n",
              "0     river    0.000000            5  ... -0.69940  1.050600  0.24051\n",
              "1  brothers    0.000000            8  ... -0.24623  0.006483 -0.21982\n",
              "2  brothers    0.050000            8  ... -0.24623  0.006483 -0.21982\n",
              "3  brothers    0.150000            8  ... -0.24623  0.006483 -0.21982\n",
              "4  brothers    0.263889            8  ... -0.24623  0.006483 -0.21982\n",
              "\n",
              "[5 rows x 105 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNNFO_uLuVz"
      },
      "source": [
        "# get data for training\n",
        "X_train = train_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_train = train_df_combined['complexity']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmE0l_emB6S"
      },
      "source": [
        "# train linear regression\n",
        "lr = LinearRegression().fit(X_train, Y_train)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luTw7T0HmB4B"
      },
      "source": [
        "# predict\n",
        "Y_pred = lr.predict(X_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHqtrYN0mB1q",
        "outputId": "206076fb-1804-4836-9169-a77b1799e525"
      },
      "source": [
        "# train loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_train[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average training absolute loss is \" + str(abl))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training absolute loss is 0.07232029925149179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "q98DV01SovTY",
        "outputId": "07b41aad-f3a5-4480-9847-205ad88fb4d3"
      },
      "source": [
        "# on test\n",
        "test_tokens = test['token'].dropna().to_list()\n",
        "test_tokens = [token.lower() for token in test_tokens] # lowercase\n",
        "print(len(test_tokens))\n",
        "\n",
        "# create a dataframe for linear regression\n",
        "test_df = pd.DataFrame(test_tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "test_df['complexity'] = test['complexity']\n",
        "\n",
        "# word length\n",
        "test_df['word_length'] = test_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = test['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count = Counter(texts)\n",
        "test_df['word_frequency'] = test_df['token'].map(lambda x: count[x])\n",
        "\n",
        "# syllables\n",
        "test_df['syllable'] = test_df['token'].map(lambda x: syllables.estimate(x))\n",
        "\n",
        "# create the weight matrix\n",
        "weights_matrix = create_weights_matrix(test_tokens)\n",
        "print(weights_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weights_matrix_df = pd.DataFrame(weights_matrix)\n",
        "test_df_combined = pd.concat([test_df, weights_matrix_df], axis=1)\n",
        "test_df_combined.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "917\n",
            "(917, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>syllable</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.25776</td>\n",
              "      <td>0.10680</td>\n",
              "      <td>-0.162650</td>\n",
              "      <td>0.42335</td>\n",
              "      <td>0.19078</td>\n",
              "      <td>0.46283</td>\n",
              "      <td>-0.959150</td>\n",
              "      <td>0.931740</td>\n",
              "      <td>0.471610</td>\n",
              "      <td>0.390770</td>\n",
              "      <td>0.54734</td>\n",
              "      <td>0.41967</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.53954</td>\n",
              "      <td>0.354970</td>\n",
              "      <td>-0.028346</td>\n",
              "      <td>0.427080</td>\n",
              "      <td>0.036569</td>\n",
              "      <td>-0.49700</td>\n",
              "      <td>-0.49543</td>\n",
              "      <td>-0.031232</td>\n",
              "      <td>-0.30298</td>\n",
              "      <td>-0.417180</td>\n",
              "      <td>-0.78459</td>\n",
              "      <td>0.70473</td>\n",
              "      <td>-0.59741</td>\n",
              "      <td>-0.33173</td>\n",
              "      <td>-0.38813</td>\n",
              "      <td>0.17189</td>\n",
              "      <td>-0.78565</td>\n",
              "      <td>-0.17219</td>\n",
              "      <td>-0.140190</td>\n",
              "      <td>0.61492</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.751090</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.60393</td>\n",
              "      <td>0.47454</td>\n",
              "      <td>0.80912</td>\n",
              "      <td>0.81709</td>\n",
              "      <td>-0.12876</td>\n",
              "      <td>-0.39310</td>\n",
              "      <td>0.17656</td>\n",
              "      <td>-0.29797</td>\n",
              "      <td>-0.32614</td>\n",
              "      <td>-0.26522</td>\n",
              "      <td>-0.37006</td>\n",
              "      <td>-0.016956</td>\n",
              "      <td>0.92268</td>\n",
              "      <td>-0.71606</td>\n",
              "      <td>-0.38524</td>\n",
              "      <td>-0.085737</td>\n",
              "      <td>0.68111</td>\n",
              "      <td>0.32080</td>\n",
              "      <td>0.45870</td>\n",
              "      <td>-0.82737</td>\n",
              "      <td>0.22932</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>-0.21221</td>\n",
              "      <td>-0.65293</td>\n",
              "      <td>-0.31427</td>\n",
              "      <td>-0.037493</td>\n",
              "      <td>0.16126</td>\n",
              "      <td>-0.46719</td>\n",
              "      <td>0.630660</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.527780</td>\n",
              "      <td>-0.34505</td>\n",
              "      <td>0.06620</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>-0.11057</td>\n",
              "      <td>-0.005771</td>\n",
              "      <td>-0.059336</td>\n",
              "      <td>0.013272</td>\n",
              "      <td>0.97305</td>\n",
              "      <td>0.454050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  105 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...        97       98        99\n",
              "0      hand    0.000000            4  ... -0.230930  0.93931  0.091475\n",
              "1      hand    0.197368            4  ... -0.230930  0.93931  0.091475\n",
              "2      hand    0.200000            4  ... -0.230930  0.93931  0.091475\n",
              "3      hand    0.267857            4  ... -0.230930  0.93931  0.091475\n",
              "4  entrance    0.000000            8  ...  0.013272  0.97305  0.454050\n",
              "\n",
              "[5 rows x 105 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEy_z413ovRJ",
        "outputId": "e1a716c4-cd21-4eca-b6b6-7b4f18222941"
      },
      "source": [
        "# get data for test\n",
        "X_test = test_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_test = test_df_combined['complexity']\n",
        "\n",
        "# predict\n",
        "Y_pred = lr.predict(X_test)\n",
        "\n",
        "# test loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_test[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average test absolute loss is \" + str(abl))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average test absolute loss is 0.0728225436388098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6xzRuFTovN5"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un9WFQkFUQcx"
      },
      "source": [
        "#LSTM classification with 5 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "hDibiYSCLuf1",
        "outputId": "d8898970-c926-41df-9d34-021496dfb506"
      },
      "source": [
        "# tokenize sentences\n",
        "\n",
        "def tokenize(sent, token, punc, stop_words):\n",
        "  \"\"\" lowercase, padded, remove stopwords and punctuations \"\"\"\n",
        "  # lowercase\n",
        "  sent = sent.lower()\n",
        "  # remove punctuation and stopwords\n",
        "  sent = ''.join([c for c in sent if c not in punc]) \n",
        "  tokens = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  # pad\n",
        "  tokens.insert(0, '<s>')\n",
        "  tokens.append('</s>')\n",
        "  # pad the token with special symbols\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == token:\n",
        "      tokens.insert(i, '_START')\n",
        "      tokens.insert(i+2, '_END')\n",
        "      break\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def get_complexity_level(n):\n",
        "  \"\"\" map complexity to corresponding level \"\"\"\n",
        "  # 1: 0, 2: 0:25, 3: 0:5, 4: 0:75, 5: 1\n",
        "  # To use cross entropy, each label has to minus 1\n",
        "  if n <= 0:\n",
        "    return 0\n",
        "  elif n <= 0.25:\n",
        "    return 1\n",
        "  elif n <= 0.5:\n",
        "    return 2\n",
        "  elif n <= 0.75:\n",
        "    return 3 \n",
        "  return 4\n",
        "\n",
        "def preprocess(df):\n",
        "  data = df[['sentence', 'token', 'complexity']]\n",
        "  data['tokenized_sentence'] = data.apply(lambda row: tokenize(row['sentence'], row['token'], punc, stop_words), axis=1)\n",
        "  data['complexity'] = data['complexity'].map(lambda x: get_complexity_level(x))\n",
        "  data = data.drop(columns=['sentence'])\n",
        "  return data\n",
        "\n",
        "train_data = preprocess(df)\n",
        "test_data = preprocess(test)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0</td>\n",
              "      <td>[&lt;s&gt;, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0</td>\n",
              "      <td>[&lt;s&gt;, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[&lt;s&gt;, put, _START, brothers, _END, far, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                   tokenized_sentence\n",
              "0     river  ...                                                         [<s>, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, </s>]\n",
              "1  brothers  ...                                                                [<s>, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, </s>]\n",
              "2  brothers  ...                  [<s>, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, </s>]\n",
              "3  brothers  ...  [<s>, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, </s>]\n",
              "4  brothers  ...                                                                                                        [<s>, put, _START, brothers, _END, far, </s>]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7zco41RQrgD"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jpf2sEdQraL",
        "outputId": "75e7bd3b-5844-4e51-af64-b8e748ed2309"
      },
      "source": [
        "# take a look at the number of instances for different complexities\n",
        "temp1 = train_data[train_data['complexity'] == 0]\n",
        "temp2 = train_data[train_data['complexity'] == 1]\n",
        "temp3 = train_data[train_data['complexity'] == 2]\n",
        "temp4 = train_data[train_data['complexity'] == 3]\n",
        "temp5 = train_data[train_data['complexity'] == 4]\n",
        "\n",
        "print(\"1: \")\n",
        "print(temp1.shape)\n",
        "print(\"2: \")\n",
        "print(temp2.shape)\n",
        "print(\"3: \")\n",
        "print(temp3.shape)\n",
        "print(\"4: \")\n",
        "print(temp4.shape)\n",
        "print(\"5: \")\n",
        "print(temp5.shape)\n",
        "\n",
        "# there are very few 1 and 5 in the training set"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: \n",
            "(18, 3)\n",
            "2: \n",
            "(3251, 3)\n",
            "3: \n",
            "(3755, 3)\n",
            "4: \n",
            "(617, 3)\n",
            "5: \n",
            "(21, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSGGceDJQo-0"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "l4fnXnBekvW3",
        "outputId": "8d5c65ff-9467-4b00-fb7e-cc919a6dc07e"
      },
      "source": [
        "# convert words to index for training and testing purpose\n",
        "sentences = train_data['tokenized_sentence'].to_list()\n",
        "temp = []\n",
        "for sent in sentences:\n",
        "  temp += sent\n",
        "temp = set(temp)\n",
        "# for words that are unknown\n",
        "temp.add('_UNKNOWN') \n",
        "temp.add('_PADDING')\n",
        "print(len(temp))\n",
        "\n",
        "# need to pad sentences to the same length\n",
        "lengths = [len(sent) for sent in sentences]\n",
        "pad_length = max(lengths)\n",
        "print(pad_length)\n",
        "\n",
        "# construct dictionaries\n",
        "word2index = {}\n",
        "index2word = {}\n",
        "for i, word in enumerate(temp):\n",
        "  word2index[word] = i\n",
        "  index2word[i] = word\n",
        "\n",
        "def word_to_index(sentence):\n",
        "  # sentence: a list of strings\n",
        "  r = []\n",
        "  for word in sentence:\n",
        "    if word in word2index:\n",
        "      r.append(word2index[word])\n",
        "    else:\n",
        "      r.append(word2index['_UNKNOWN'])\n",
        "  diff = pad_length - len(sentence)\n",
        "  pad_index = word2index['_PADDING']\n",
        "  for i in range(diff):\n",
        "    r.append(pad_index)\n",
        "  return r\n",
        "\n",
        "train_data['number_sentence'] = train_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "test_data['number_sentence'] = test_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "\n",
        "train_data = train_data.drop(columns=['tokenized_sentence'])\n",
        "test_data = test_data.drop(columns=['tokenized_sentence'])\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14826\n",
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>number_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0</td>\n",
              "      <td>[12519, 10247, 7219, 13783, 13286, 14669, 1919, 13564, 9596, 6190, 6534, 1805, 8725, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0</td>\n",
              "      <td>[12519, 1320, 1563, 13783, 1976, 14669, 5114, 7041, 5972, 2765, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[12519, 9006, 9305, 4797, 10670, 5002, 604, 7164, 2595, 4708, 4233, 13783, 1976, 14669, 7246, 2194, 7736, 12769, 7670, 8125, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[12519, 4683, 5221, 2810, 3235, 10720, 13783, 1976, 14669, 5712, 12226, 7183, 12823, 13593, 6496, 1378, 7183, 9567, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[12519, 2253, 13783, 1976, 14669, 9750, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                number_sentence\n",
              "0     river  ...      [12519, 10247, 7219, 13783, 13286, 14669, 1919, 13564, 9596, 6190, 6534, 1805, 8725, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "1  brothers  ...      [12519, 1320, 1563, 13783, 1976, 14669, 5114, 7041, 5972, 2765, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "2  brothers  ...               [12519, 9006, 9305, 4797, 10670, 5002, 604, 7164, 2595, 4708, 4233, 13783, 1976, 14669, 7246, 2194, 7736, 12769, 7670, 8125, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "3  brothers  ...          [12519, 4683, 5221, 2810, 3235, 10720, 13783, 1976, 14669, 5712, 12226, 7183, 12823, 13593, 6496, 1378, 7183, 9567, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "4  brothers  ...  [12519, 2253, 13783, 1976, 14669, 9750, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNw9bjg2Luda",
        "outputId": "18eaa7a7-e0e7-4196-e6c9-4ed8fdcd1b90"
      },
      "source": [
        "# do a simple check\n",
        "print(df.shape)\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(len(word2index.keys()))\n",
        "print(len(index2word.keys()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 5)\n",
            "(7662, 3)\n",
            "(917, 3)\n",
            "14826\n",
            "14826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3ASkWeeGZo",
        "outputId": "c86838b6-94aa-405d-ee15-e2d12b0529b9"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTh_SM0eGXC"
      },
      "source": [
        "# prepare the batch loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOaqYVuaDuQ_"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word, adding word length, frequency and syllables \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 3))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      # initialize a random vector\n",
        "      weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word], syllables.estimate(word)])), axis=0 ) # concatenate 2 1d arrays\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmDPzyUOeGUo"
      },
      "source": [
        "# the LSTM class\n",
        "class ComplexityNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)       \n",
        "        out = self.fc(lstm_out[:, -1, :])      \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFAZtgekeGSh",
        "outputId": "814abb57-5099-4db7-f6de-02b8dd03ec03"
      },
      "source": [
        "# some parameters\n",
        "vocab_size = len(word2index) + 1\n",
        "output_size = 5\n",
        "# embedding dim is not needed because it can be obtained from weights_matrix\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNet(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNet(\n",
            "  (embedding): Embedding(14826, 103)\n",
            "  (lstm): LSTM(103, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eggcDcSeGQJ",
        "outputId": "066847e4-3008-46b9-ed73-97daabf6c7ab"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 1.021214...\n",
            "Epoch: 9/100... Step: 512... Loss: 1.053857...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.951657...\n",
            "Epoch: 18/100... Step: 1024... Loss: 1.055660...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.936611...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.986246...\n",
            "Epoch: 31/100... Step: 1792... Loss: 1.048536...\n",
            "Epoch: 35/100... Step: 2048... Loss: 1.049713...\n",
            "Epoch: 40/100... Step: 2304... Loss: 1.035149...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.917061...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.918828...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.895716...\n",
            "Epoch: 57/100... Step: 3328... Loss: 1.014937...\n",
            "Epoch: 61/100... Step: 3584... Loss: 1.017656...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.964298...\n",
            "Epoch: 70/100... Step: 4096... Loss: 1.026663...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.934233...\n",
            "Epoch: 79/100... Step: 4608... Loss: 1.028960...\n",
            "Epoch: 83/100... Step: 4864... Loss: 1.163527...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.964568...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.960990...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.949624...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.899107...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1997iM_psXaP"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_classification_no_Glove.pt\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASr80p94sXPA",
        "outputId": "f28c1edf-3f54-4577-c579-e2000e98eb6f"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average; different from the cross entropy used in training\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - ( fetched_output[i].index( max(fetched_output[i])) ) )\n",
        "      if diff == 0:\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 2, 0, 2, 1, 0, 1, 1, 0, 0, 2, 0, 1, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 2, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 1, 2, 1, 0, 2, 0, 1, 0, 3, 2, 0, 1, 2, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 2, 0, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 2, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 2, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 0, 1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 2, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 2, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 2, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1, 2, 1, 2, 2, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 2, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 2, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 0, 1, 2, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 2, 0, 2, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2, 0, 1, 1, 1, 0, 1, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1]\n",
            "Test loss: 0.646\n",
            "Test accuracy: 42.748%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w52OdjZtBS_"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29TcMd795Kyg"
      },
      "source": [
        "#LSTM classification with 5 classes and pre-trained Glove embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXb6AzSzSYJT"
      },
      "source": [
        "# restart the loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zi2FuT5tBQn"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in glove, adding word length, frequency and syllables\"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 3))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = torch.cat( (glove[word], torch.Tensor([len(word), count_word_frequency[word], syllables.estimate(word)])) ) \n",
        "      except KeyError:\n",
        "          # initialize a random vector\n",
        "          weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word], syllables.estimate(word)])), axis=0 ) # concatenate 2 1d arrays\n",
        "      \n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3EQOW-g5LON"
      },
      "source": [
        "# The model; inherits from the previous model\n",
        "class ComplexityNetGlove(ComplexityNet):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNetGlove, self).__init__(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True) # use the Glove\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyFDmkvv5pQ1",
        "outputId": "9f3931d0-d38c-43ce-ef5f-7bcf99e5a910"
      },
      "source": [
        "vocab_size = len(word2index) + 1\n",
        "output_size = 5\n",
        "\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNetGlove(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNetGlove(\n",
            "  (embedding): Embedding(14826, 103)\n",
            "  (lstm): LSTM(103, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F9qH-4v5pGn",
        "outputId": "36c45251-bac8-4a80-a280-68880b45095a"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 1.045902...\n",
            "Epoch: 9/100... Step: 512... Loss: 1.043418...\n",
            "Epoch: 14/100... Step: 768... Loss: 1.035378...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.948163...\n",
            "Epoch: 22/100... Step: 1280... Loss: 1.068360...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.955419...\n",
            "Epoch: 31/100... Step: 1792... Loss: 1.003160...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.992437...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.964292...\n",
            "Epoch: 44/100... Step: 2560... Loss: 1.016639...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.876871...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.955015...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.941595...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.974338...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.925720...\n",
            "Epoch: 70/100... Step: 4096... Loss: 1.007498...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.994998...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.876456...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.951598...\n",
            "Epoch: 87/100... Step: 5120... Loss: 1.006770...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.896134...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.907851...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.981545...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOadn6Uv5o_G"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_classification_with_Glove.pt\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPj_sgcM7oJ2",
        "outputId": "442f0e0d-757e-47fa-d762-83f6c6c0f75f"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - ( fetched_output[i].index( max(fetched_output[i])) ) )\n",
        "      if diff == 0:\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 2, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 2, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0]\n",
            "Test loss: 0.535\n",
            "Test accuracy: 46.020%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kojg1RJswgx"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpmgJ_Y7tEWe"
      },
      "source": [
        "# LSTM with continuous complexities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWKa5VjeyJsA"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "DGZCGk8eyKLE",
        "outputId": "16b43444-a15f-4aac-8fef-3360cbdbf15a"
      },
      "source": [
        "# tokenize sentences\n",
        "\n",
        "def tokenize(sent, token, punc, stop_words):\n",
        "  \"\"\" lowercase, padded, remove stopwords and punctuations \"\"\"\n",
        "  # lowercase\n",
        "  sent = sent.lower()\n",
        "  # remove punctuation and stopwords\n",
        "  sent = ''.join([c for c in sent if c not in punc]) \n",
        "  tokens = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  # pad\n",
        "  tokens.insert(0, '<s>')\n",
        "  tokens.append('</s>')\n",
        "  # pad the token with special symbols\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == token:\n",
        "      tokens.insert(i, '_START')\n",
        "      tokens.insert(i+2, '_END')\n",
        "      break\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def preprocess(df):\n",
        "  data = df[['sentence', 'token', 'complexity']]\n",
        "  data['tokenized_sentence'] = data.apply(lambda row: tokenize(row['sentence'], row['token'], punc, stop_words), axis=1)\n",
        "  data = data.drop(columns=['sentence'])\n",
        "  return data\n",
        "\n",
        "train_data = preprocess(df)\n",
        "test_data = preprocess(test)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[&lt;s&gt;, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[&lt;s&gt;, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>[&lt;s&gt;, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>[&lt;s&gt;, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>[&lt;s&gt;, put, _START, brothers, _END, far, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                   tokenized_sentence\n",
              "0     river  ...                                                         [<s>, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, </s>]\n",
              "1  brothers  ...                                                                [<s>, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, </s>]\n",
              "2  brothers  ...                  [<s>, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, </s>]\n",
              "3  brothers  ...  [<s>, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, </s>]\n",
              "4  brothers  ...                                                                                                        [<s>, put, _START, brothers, _END, far, </s>]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "c4XYi3COyKLF",
        "outputId": "7f8ba074-2046-4e62-d3a5-a2b0d8364652"
      },
      "source": [
        "# convert words to index for training and testing purpose\n",
        "sentences = train_data['tokenized_sentence'].to_list()\n",
        "temp = []\n",
        "for sent in sentences:\n",
        "  temp += sent\n",
        "temp = set(temp)\n",
        "# for words that are unknown\n",
        "temp.add('_UNKNOWN') \n",
        "temp.add('_PADDING')\n",
        "print(len(temp))\n",
        "\n",
        "# need to pad sentences to the same length\n",
        "lengths = [len(sent) for sent in sentences]\n",
        "pad_length = max(lengths)\n",
        "print(pad_length)\n",
        "\n",
        "# construct dictionaries\n",
        "word2index = {}\n",
        "index2word = {}\n",
        "for i, word in enumerate(temp):\n",
        "  word2index[word] = i\n",
        "  index2word[i] = word\n",
        "\n",
        "def word_to_index(sentence):\n",
        "  # sentence: a list of strings\n",
        "  r = []\n",
        "  for word in sentence:\n",
        "    if word in word2index:\n",
        "      r.append(word2index[word])\n",
        "    else:\n",
        "      r.append(word2index['_UNKNOWN'])\n",
        "  diff = pad_length - len(sentence)\n",
        "  pad_index = word2index['_PADDING']\n",
        "  for i in range(diff):\n",
        "    r.append(pad_index)\n",
        "  return r\n",
        "\n",
        "train_data['number_sentence'] = train_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "test_data['number_sentence'] = test_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "\n",
        "train_data = train_data.drop(columns=['tokenized_sentence'])\n",
        "test_data = test_data.drop(columns=['tokenized_sentence'])\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14826\n",
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>number_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[12519, 10247, 7219, 13783, 13286, 14669, 1919, 13564, 9596, 6190, 6534, 1805, 8725, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[12519, 1320, 1563, 13783, 1976, 14669, 5114, 7041, 5972, 2765, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>[12519, 9006, 9305, 4797, 10670, 5002, 604, 7164, 2595, 4708, 4233, 13783, 1976, 14669, 7246, 2194, 7736, 12769, 7670, 8125, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>[12519, 4683, 5221, 2810, 3235, 10720, 13783, 1976, 14669, 5712, 12226, 7183, 12823, 13593, 6496, 1378, 7183, 9567, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>[12519, 2253, 13783, 1976, 14669, 9750, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                number_sentence\n",
              "0     river  ...      [12519, 10247, 7219, 13783, 13286, 14669, 1919, 13564, 9596, 6190, 6534, 1805, 8725, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "1  brothers  ...      [12519, 1320, 1563, 13783, 1976, 14669, 5114, 7041, 5972, 2765, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "2  brothers  ...               [12519, 9006, 9305, 4797, 10670, 5002, 604, 7164, 2595, 4708, 4233, 13783, 1976, 14669, 7246, 2194, 7736, 12769, 7670, 8125, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "3  brothers  ...          [12519, 4683, 5221, 2810, 3235, 10720, 13783, 1976, 14669, 5712, 12226, 7183, 12823, 13593, 6496, 1378, 7183, 9567, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "4  brothers  ...  [12519, 2253, 13783, 1976, 14669, 9750, 2993, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, 10288, ...]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs1y-euItBtz"
      },
      "source": [
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-SaakyDHMyu"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word, adding word length, frequency and syllables \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 3))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      # initialize a random vector\n",
        "      weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word], syllables.estimate(word)])), axis=0 ) # concatenate 2 1d arrays\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXeyJqECtBt0"
      },
      "source": [
        "# the LSTM class\n",
        "class ComplexityNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)       \n",
        "        out = self.fc(lstm_out[:, -1, :])      \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3nsx9L9tBt1",
        "outputId": "5fa6226b-a980-4548-d2a3-f24fe8a37374"
      },
      "source": [
        "# some parameters\n",
        "vocab_size = len(word2index) + 1\n",
        "output_size = 1\n",
        "\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNet(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNet(\n",
            "  (embedding): Embedding(14826, 103)\n",
            "  (lstm): LSTM(103, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8vIp2rStBt1",
        "outputId": "b33eebd4-d811-44b4-9f63-79e6de24406f"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.110809...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.112754...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.087854...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.104705...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.106671...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.095664...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.116617...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.115502...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.123669...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.093843...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.083815...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.095055...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.108419...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.097134...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.106839...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.110869...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.103946...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.106590...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.104640...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.099286...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.095802...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.100591...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.108255...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcTUYus9tBt2"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_continuous_no_Glove.pt\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSpouH1tBt2",
        "outputId": "5883e356-8d5e-4c7a-c881-24cf1838eff8"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average; different from the cross entropy used in training\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - fetched_output[i] )\n",
        "      if diff <= 0.05: # a difference that is small\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.08369353413581848, 0.020473132530848148, 0.09063797858026287, 0.0055685341358184814, 0.09472294590052438, 0.07255646586418152, 0.07404456110227675, 0.39547313253084815, 0.035082423024707377, 0.29825091030862594, 0.2141005835112404, 0.014520751578467217, 0.057753834285234174, 0.4788064658641815, 0.18786020080248517, 0.2211935341358185, 0.006487651782877357, 0.016306465864181496, 0.028806465864181563, 0.006584243641959309, 0.0343620214197371, 0.08786020080248516, 0.11213979919751482, 0.0055685341358184814, 0.03762999527594624, 0.05060529884170084, 0.02119353413581848, 0.02119353413581848, 0.05452686746915181, 0.1954731325308482, 0.07255646586418152, 0.18190781985010418, 0.02119353413581848, 0.14619353413581848, 0.12166360872132442, 0.12011081369026844, 0.008218230570063934, 0.10380646586418152, 0.17880646586418153, 0.022924112923005002, 0.02119353413581848, 0.046193534135818476, 0.06806853413581848, 0.14057117174653444, 0.1176953547530703, 0.08174764233476972, 0.03786020080248517, 0.03905067699296133, 0.02119353413581848, 0.1954731325308482, 0.0670417599818286, 0.02119353413581848, 0.02119353413581848, 0.1045268674691518, 0.17880646586418153, 0.10380646586418152, 0.1954731325308482, 0.29547313253084817, 0.34419108124879694, 0.035082423024707377, 0.05690781985010418, 0.05965507259735689, 0.07674908969137409, 0.0697155567732724, 0.035082423024707377, 0.004526867469151763, 0.03435142887266057, 0.0051222553378657, 0.09931853413581848, 0.035082423024707377, 0.0033363912786755767, 0.02119353413581848, 0.21793690064679022, 0.13505646586418152, 0.08369353413581848, 0.022924112923005002, 0.018280150074707846, 0.2756814658641815, 0.02568146586418152, 0.1711935341358185, 0.27426101131872693, 0.028806465864181563, 0.02119353413581848, 0.05244353413581848, 0.05380646586418153, 0.08369353413581848, 0.07382511308318687, 0.1711935341358185, 0.05452686746915181, 0.09619353413581849, 0.02119353413581848, 0.06880646586418154, 0.18468881880535803, 0.04897131191359627, 0.08369353413581848, 0.0033363912786755767, 0.0788064658641815, 0.13505646586418152, 0.07404456110227675, 0.007304645246929642, 0.25380646586418154, 0.09244282950054522, 0.02568146586418152, 0.05060529884170084, 0.07119353413581847, 0.04547313253084817, 0.4538064658641815, 0.045003057945342345, 0.04825091030862594, 0.2913064658641815, 0.2434157563580407, 0.03309829604058037, 0.0788064658641815, 0.03786020080248517, 0.07476496270724703, 0.0670417599818286, 0.04750932360950272, 0.004526867469151763, 0.08818146586418152, 0.04698828404599975, 0.2756814658641815, 0.14619353413581848, 0.05452686746915181, 0.11494353413581848, 0.05690781985010418, 0.08786020080248516, 0.058351920409636104, 0.08594932300703861, 0.11630646586418153, 0.03681853413581848, 0.08594932300703861, 0.22880646586418152, 0.014520751578467217, 0.05380646586418153, 0.10380646586418152, 0.14619353413581848, 0.006584243641959309, 0.05244353413581848, 0.06066721834634481, 0.035082423024707377, 0.272924112923005, 0.1699829364524168, 0.11047924842153278, 0.09472294590052438, 0.02568146586418152, 0.028806465864181563, 0.022924112923005002, 0.012139799197514844, 0.05233587762888736, 0.02119353413581848, 0.012139799197514844, 0.02119353413581848, 0.11047924842153278, 0.08991757697529273, 0.3538064658641815, 0.02119353413581848, 0.1086935341358185, 0.17523503729275292, 0.06213979919751489, 0.22880646586418152, 0.09931853413581848, 0.13056853413581848, 0.15068146586418152, 0.03762999527594624, 0.17325091030862594, 0.0038064658641815408, 0.2211935341358185, 0.11841575635804069, 0.07119353413581847, 0.11943146586418152, 0.07476496270724703, 0.12119353413581846, 0.24130646586418147, 0.05244353413581848, 0.03435142887266057, 0.1045268674691518, 0.06066721834634481, 0.24351234821712264, 0.3538064658641815, 0.06809218014989582, 0.09262210556438993, 0.3209117290220762, 0.0670417599818286, 0.04825091030862594, 0.3663064658641815, 0.06213979919751489, 0.02119353413581848, 0.09547313253084821, 0.12833639127867563, 0.06531118119464199, 0.03435142887266057, 0.02119353413581848, 0.2211935341358185, 0.2823778944356101, 0.05452686746915181, 0.04825091030862594, 0.012139799197514844, 0.028806465864181563, 0.05060529884170084, 0.00803563939897639, 0.45102868808640373, 0.2565842436419593, 0.08991757697529273, 0.05244353413581848, 0.04042430336658767, 0.24443146586418152, 0.08174764233476972, 0.18193146586418152, 0.07119353413581847, 0.09619353413581849, 0.1315842436419593, 0.03369353413581849, 0.03681853413581848, 0.06213979919751489, 0.06531118119464199, 0.024261011318726933, 0.18468881880535803, 0.25380646586418154, 0.09262210556438993, 0.1711935341358185, 0.02119353413581848, 0.1045268674691518, 0.14619353413581848, 0.13670120270628683, 0.04130646586418152, 0.17880646586418153, 0.02119353413581848, 0.17880646586418153, 0.055729542787258446, 0.04130646586418152, 0.08786020080248516, 0.0055685341358184814, 0.028806465864181563, 0.03589941648875969, 0.16213979919751487, 0.06213979919751489, 0.11841575635804069, 0.14057117174653444, 0.19619353413581847, 0.04897131191359627, 0.16213979919751487, 0.03309829604058037, 0.02119353413581848, 0.07119353413581847, 0.11841575635804069, 0.16630646586418152, 0.08369353413581848, 0.07476496270724703, 0.07382511308318687, 0.0033363912786755767, 0.18193146586418152, 0.018280150074707846, 0.08594932300703861, 0.05244353413581848, 0.02119353413581848, 0.04897131191359627, 0.05693146586418152, 0.0055685341358184814, 0.4788064658641815, 0.1323046452469296, 0.09619353413581849, 0.12586528939359332, 0.06531118119464199, 0.04130646586418152, 0.04825091030862594, 0.06806853413581848, 0.09262210556438993, 0.02119353413581848, 0.15737789443561012, 0.02119353413581848, 0.018280150074707846, 0.016306465864181496, 0.10380646586418152, 0.0051222553378657, 0.008218230570063934, 0.02119353413581848, 0.10380646586418152, 0.0628602008024852, 0.11494353413581848, 0.006584243641959309, 0.19619777021200763, 0.03786020080248517, 0.03833027538799105, 0.010056465864181519, 0.02119353413581848, 0.0670417599818286, 0.0670417599818286, 0.21491757697529273, 0.11841575635804069, 0.08369353413581848, 0.03681853413581848, 0.04897131191359627, 0.020473132530848148, 0.11047924842153278, 0.008218230570063934, 0.04897131191359627, 0.03762999527594624, 0.035082423024707377, 0.04897131191359627, 0.09645352468771096, 0.05233587762888736, 0.12880646586418154, 0.02119353413581848, 0.14619353413581848, 0.03905067699296133, 0.3121397991975149, 0.02568146586418152, 0.2600564658641815, 0.05690781985010418, 0.2211935341358185, 0.16405067699296133, 0.272924112923005, 0.06213979919751489, 0.3038064658641815, 0.06531118119464199, 0.012139799197514844, 0.06630646586418154, 0.1711935341358185, 0.12833639127867563, 0.14619353413581848, 0.022924112923005002, 0.1711935341358185, 0.004526867469151763, 0.006584243641959309, 0.0670417599818286, 0.09547313253084821, 0.05233587762888736, 0.08698300782002899, 0.03762999527594624, 0.03369353413581849, 0.10073898868127301, 0.05244353413581848, 0.03905067699296133, 0.10380646586418152, 0.12586528939359332, 0.009288772231056619, 0.04130646586418152, 0.0800170635475832, 0.05244353413581848, 0.32404456110227675, 0.02119353413581848, 0.13505646586418152, 0.13670120270628683, 0.08818146586418152, 0.21318146586418152, 0.26213979919751484, 0.04547313253084817, 0.06213979919751489, 0.19619353413581847, 0.05690781985010418, 0.0788064658641815, 0.006487651782877357, 0.02119353413581848, 0.06531118119464199, 0.14619353413581848, 0.26213979919751484, 0.12119353413581846, 0.05244353413581848, 0.20452686746915183, 0.16630646586418152, 0.08991757697529273, 0.03833027538799105, 0.0628602008024852, 0.035624647682363286, 0.09547313253084821, 0.09472294590052438, 0.05380646586418153, 0.03143804481154999, 0.024261011318726933, 0.05060529884170084, 0.06531118119464199, 0.010056465864181519, 0.09130646586418151, 0.07119353413581847, 0.10942882825346556, 0.15452686746915184, 0.009829897772182128, 0.1454731325308481, 0.06213979919751489, 0.05452686746915181, 0.11329879729371323, 0.02119353413581848, 0.0343620214197371, 0.009288772231056619, 0.016306465864181496, 0.2645207515784672, 0.020473132530848148, 0.008218230570063934, 0.05060529884170084, 0.21491757697529273, 0.006584243641959309, 0.2211935341358185, 0.03649877355648923, 0.11494353413581848, 0.11943146586418152, 0.035082423024707377, 0.08786020080248516, 0.08786020080248516, 0.08369353413581848, 0.0015337385914542256, 0.022924112923005002, 0.0800170635475832, 0.10942882825346556, 0.0788064658641815, 0.02119353413581848, 0.2645207515784672, 0.09063797858026287, 0.009829897772182128, 0.03589941648875969, 0.07674908969137409, 0.15068146586418152, 0.06531118119464199, 0.02119353413581848, 0.14057117174653444, 0.4788064658641815, 0.2600564658641815, 0.03143804481154999, 0.03762999527594624, 0.2211935341358185, 0.06806853413581848, 0.12586528939359332, 0.050235037292752915, 0.014520751578467217, 0.11115940704065208, 0.0440238571685293, 0.0033363912786755767, 0.05690781985010418, 0.11047924842153278, 0.1045268674691518, 0.08786020080248516, 0.10380646586418152, 0.02568146586418152, 0.1711935341358185, 0.06881258175486607, 0.03435142887266057, 0.24269535475307036, 0.010056465864181519, 0.02119353413581848, 0.12119353413581846, 0.19309218014989582, 0.3538064658641815, 0.15354647531228904, 0.18786020080248517, 0.04547313253084817, 0.010056465864181519, 0.19619777021200763, 0.07091172902207621, 0.09645352468771096, 0.22880646586418152, 0.0055685341358184814, 0.20102868808640373, 0.11943146586418152, 0.5065842436419592, 0.11047924842153278, 0.15527705409947568, 0.05693146586418152, 0.004526867469151763, 0.02119353413581848, 0.4052770540994757, 0.08594932300703861, 0.00869353413581847, 0.04130646586418152, 0.03369353413581849, 0.3170417599818286, 0.07663255282070319, 0.05060529884170084, 0.0051222553378657, 0.16008242302470738, 0.05452686746915181, 0.2711935341358185, 0.17617488691681304, 0.07476496270724703, 0.018280150074707846, 0.03435142887266057, 0.09472294590052438, 0.05452686746915181, 0.0033363912786755767, 0.2565842436419593, 0.3663064658641815, 0.04897131191359627, 0.21318146586418152, 0.04459593954839203, 0.08369353413581848, 0.02119353413581848, 0.07255646586418152, 0.12413471060640674, 0.08818146586418152, 0.09244282950054522, 0.05244353413581848, 0.17523503729275292, 0.10380646586418152, 0.05060529884170084, 0.046193534135818476, 0.05060529884170084, 0.04825091030862594, 0.02119353413581848, 0.09931853413581848, 0.08698300782002899, 0.014520751578467217, 0.20102868808640373, 0.02119353413581848, 0.04897131191359627, 0.08369353413581848, 0.05690781985010418, 0.05380222978799237, 0.1670268674691518, 0.05380646586418153, 0.24196436060102366, 0.24443146586418152, 0.03589941648875969, 0.04459593954839203, 0.08369353413581848, 0.2711935341358185, 0.21762210556438993, 0.03435142887266057, 0.1413064658641815, 0.05452686746915181, 0.1593620214197371, 0.02119353413581848, 0.15737789443561012, 0.1045268674691518, 0.03681853413581848, 0.09063797858026287, 0.03905067699296133, 0.14619353413581848, 0.3002350372927529, 0.30233587762888736, 0.0670417599818286, 0.04897131191359627, 0.11213979919751482, 0.1811874182451338, 0.03369353413581849, 0.03681853413581848, 0.18468881880535803, 0.004526867469151763, 0.13789737495509058, 0.008218230570063934, 0.00803563939897639, 0.02119353413581848, 0.08698300782002899, 0.08369353413581848, 0.09645352468771096, 0.08594932300703861, 0.2711935341358185, 0.14619353413581848, 0.2434157563580407, 0.07476496270724703, 0.0343620214197371, 0.15580891875120312, 0.020473132530848148, 0.20249067639049734, 0.3288064658641815, 0.17880646586418153, 0.22880646586418152, 0.17880646586418153, 0.006584243641959309, 0.028806465864181563, 0.08107919313690881, 0.04547313253084817, 0.05452686746915181, 0.1954731325308482, 0.07119353413581847, 0.10014090255687114, 0.09931853413581848, 0.0670417599818286, 0.03681853413581848, 0.02119353413581848, 0.03786020080248517, 0.02119353413581848, 0.08369353413581848, 0.04130646586418152, 0.016306465864181496, 0.12119353413581846, 0.05244353413581848, 0.06531118119464199, 0.21564857112733932, 0.08174764233476972, 0.09619353413581849, 0.15527705409947568, 0.0055685341358184814, 0.03681853413581848, 0.0038064658641815408, 0.0628602008024852, 0.0026159896737053545, 0.09619353413581849, 0.2121397991975148, 0.03681853413581848, 0.0788064658641815, 0.26213979919751484, 0.018280150074707846, 0.08369353413581848, 0.1454731325308481, 0.1606246476823633, 0.07255646586418152, 0.08991757697529273, 0.12586528939359332, 0.24196436060102366, 0.09619353413581849, 0.12880646586418154, 0.3121397991975149, 0.02119353413581848, 0.05690781985010418, 0.035082423024707377, 0.04130646586418152, 0.3399175769752927, 0.0003602008024851111, 0.04825091030862594, 0.29547313253084817, 0.03681853413581848, 0.14619353413581848, 0.02119353413581848, 0.28315429195113806, 0.04042430336658767, 0.006487651782877357, 0.2600564658641815, 0.010056465864181519, 0.05693146586418152, 0.06809218014989582, 0.02568146586418152, 0.04750932360950272, 0.16008242302470738, 0.1954731325308482, 0.04130646586418152, 0.07602868808640373, 0.08991757697529273, 0.050235037292752915, 0.19309218014989582, 0.11494353413581848, 0.24666360872132442, 0.19755646586418152, 0.05452686746915181, 0.09547313253084821, 0.04547313253084817, 0.42325091030862594, 0.3038064658641815, 0.00803563939897639, 0.05452686746915181, 0.09262210556438993, 0.19619353413581847, 0.14057117174653444, 0.05965507259735689, 0.13786020080248512, 0.16630646586418152, 0.02119353413581848, 0.07476496270724703, 0.03088979919751489, 0.05452686746915181, 0.06630646586418154, 0.0055685341358184814, 0.08991757697529273, 0.07382511308318687, 0.24443146586418152, 0.11115940704065208, 0.11047924842153278, 0.006487651782877357, 0.1538064658641815, 0.15527705409947568, 0.06806853413581848, 0.012139799197514844, 0.0972275184957605, 0.10942882825346556, 0.07119353413581847, 0.02119353413581848, 0.0670417599818286, 0.02119353413581848, 0.02119353413581848, 0.2121397991975148, 0.006584243641959309, 0.046193534135818476, 0.03143804481154999, 0.02119353413581848, 0.012139799197514844, 0.08369353413581848, 0.04897131191359627, 0.07476496270724703, 0.06806853413581848, 0.1593620214197371, 0.0038064658641815408, 0.4609493230070386, 0.26726800432571995, 0.03905067699296133, 0.40380646586418156, 0.0343620214197371, 0.04130646586418152, 0.11115940704065208, 0.04825091030862594, 0.06881258175486607, 0.12413471060640674, 0.11494353413581848, 0.02119353413581848, 0.05869353413581849, 0.1045268674691518, 0.026425513483229246, 0.3716636087213244, 0.19619353413581847, 0.04130646586418152, 0.026425513483229246, 0.050235037292752915, 0.24443146586418152, 0.03681853413581848, 0.02119353413581848, 0.06213979919751489, 0.07496031201802761, 0.0033363912786755767, 0.19619353413581847, 0.14619353413581848, 0.11422313253084809, 0.009288772231056619, 0.0800170635475832, 0.09244282950054522, 0.0051222553378657, 0.06066721834634481, 0.14619353413581848, 0.07674908969137409, 0.050235037292752915, 0.020473132530848148, 0.0670417599818286, 0.20102868808640373, 0.15068146586418152, 0.04897131191359627, 0.09547313253084821, 0.02119353413581848, 0.2600564658641815, 0.08786020080248516, 0.058806465864181534, 0.07119353413581847, 0.22880646586418152, 0.03786020080248517, 0.0800170635475832, 0.04130646586418152, 0.06213979919751489, 0.05869353413581849, 0.2565842436419593, 0.10014090255687114, 0.04459593954839203, 0.00803563939897639, 0.05380646586418153, 0.006487651782877357, 0.024261011318726933, 0.0343620214197371, 0.15737789443561012, 0.07119353413581847, 0.07119353413581847, 0.12833639127867563, 0.11494353413581848, 0.22880646586418152, 0.03681853413581848, 0.04897131191359627, 0.1323046452469296, 0.03237789443561012, 0.04459593954839203, 0.02568146586418152, 0.22880646586418152, 0.06806853413581848, 0.03435142887266057, 0.12413471060640674, 0.15068146586418152, 0.3069314658641815, 0.13884059295934792, 0.05233587762888736, 0.07496031201802761, 0.0788064658641815, 0.07382511308318687, 0.05452686746915181, 0.02568146586418152, 0.03762999527594624, 0.07888584182812619, 0.2711935341358185, 0.05693146586418152, 0.02119353413581848, 0.08786020080248516, 0.3180921801498958, 0.09244282950054522, 0.03905067699296133, 0.05244353413581848, 0.035082423024707377, 0.05060529884170084, 0.0038064658641815408, 0.0788064658641815, 0.11494353413581848, 0.07119353413581847, 0.02119353413581848, 0.07801171595400033, 0.14619353413581848, 0.4319314658641815, 0.07602868808640373, 0.03786020080248517, 0.065762987603312, 0.09645352468771096, 0.03762999527594624, 0.07382511308318687, 0.03237789443561012, 0.05693146586418152, 0.10380646586418152, 0.008218230570063934, 0.007304645246929642, 0.12119353413581846, 0.10380646586418152, 0.018280150074707846, 0.26726800432571995, 0.035082423024707377, 0.04547313253084817, 0.13786020080248512, 0.07602868808640373, 0.09931853413581848, 0.12880646586418154, 0.05060529884170084, 0.13505646586418152, 0.0343620214197371, 0.05060529884170084, 0.006584243641959309, 0.028806465864181563, 0.018280150074707846, 0.03905067699296133, 0.028806465864181563, 0.12833639127867563, 0.05060529884170084, 0.032557170499454835, 0.11494353413581848, 0.035082423024707377, 0.0038064658641815408, 0.0670417599818286, 0.07404456110227675, 0.035082423024707377, 0.02119353413581848, 0.006487651782877357, 0.07602868808640373, 0.02119353413581848, 0.2141005835112404, 0.12413471060640674, 0.10380646586418152, 0.028806465864181563, 0.08991757697529273, 0.06806853413581848, 0.04547313253084817, 0.0343620214197371, 0.07382511308318687, 0.24547313253084824, 0.008218230570063934, 0.004526867469151763, 0.02119353413581848, 0.10380646586418152, 0.0033363912786755767, 0.3381814658641815, 0.07602868808640373, 0.09645352468771096, 0.05690781985010418, 0.0038064658641815408, 0.24351234821712264, 0.1454731325308481, 0.08174764233476972, 0.28762999527594624, 0.03589941648875969, 0.0972275184957605, 0.05233587762888736, 0.03369353413581849, 0.19755646586418152, 0.11115940704065208, 0.06531118119464199, 0.08786020080248516, 0.035082423024707377, 0.0005455962989641128, 0.057753834285234174, 0.08786020080248516, 0.12880646586418154, 0.02119353413581848, 0.12119353413581846, 0.07476496270724703, 0.02119353413581848, 0.020473132530848148, 0.04611415817187381, 0.02119353413581848, 0.06806853413581848, 0.13505646586418152, 0.007304645246929642, 0.1454731325308481, 0.07382511308318687, 0.18190781985010418, 0.3002350372927529, 0.19309218014989582, 0.2038064658641815, 0.1045268674691518, 0.04611415817187381, 0.11038541323260254, 0.15354647531228904, 0.03237789443561012, 0.02119353413581848, 0.09262210556438993, 0.16405067699296133, 0.09063797858026287, 0.010056465864181519, 0.09262210556438993, 0.07119353413581847, 0.08786020080248516, 0.22880646586418152, 0.13505646586418152]\n",
            "Test loss: 0.097\n",
            "Test accuracy: 34.787%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyWnPv58tKww"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxR1pjShtLEg"
      },
      "source": [
        "#LSTM with continuous complexities and with pre-trained Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grGcPB_Tt4Bi"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSacas2at8FS"
      },
      "source": [
        "# restart the loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar2O53uot8FS"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in glove, adding word length, frequency and syllables \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 3))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = torch.cat( (glove[word], torch.Tensor([len(word), count_word_frequency[word], syllables.estimate(word)])) ) \n",
        "      except KeyError:\n",
        "          # initialize a random vector\n",
        "          weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word], syllables.estimate(word)])), axis=0 ) # concatenate 2 1d arrays\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxCq01M0t8FS"
      },
      "source": [
        "# The model; inherits from the previous model\n",
        "class ComplexityNetGlove(ComplexityNet):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNetGlove, self).__init__(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True) # use the Glove\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfdiMdSPt8FT",
        "outputId": "6d78cb8c-b748-4a53-cf15-e0b7040f8aeb"
      },
      "source": [
        "vocab_size = len(word2index) + 1\n",
        "output_size = 1\n",
        "\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNetGlove(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNetGlove(\n",
            "  (embedding): Embedding(14826, 103)\n",
            "  (lstm): LSTM(103, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUujtF0At8FT",
        "outputId": "5baec5ca-c8f0-4d10-8740-6d515b7ff9a5"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.099750...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.106811...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.096551...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.103706...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.094218...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.106070...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.100953...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.104744...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.099212...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.097646...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.110612...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.106337...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.106714...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.086385...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.109730...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.102852...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.100007...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.107177...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.115883...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.109049...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.112429...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.115575...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.104158...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9me9Lr8mt8FT"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_continuous_with_Glove.pt\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnw42ciMt8FU",
        "outputId": "d1a5475d-4bb5-4fd0-8b39-55ab2738b785"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - fetched_output[i] )\n",
        "      if diff <= 0.05: # a difference that is small\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.2586483889155917, 0.009465352577321617, 0.028092833360036207, 0.1263384771974463, 0.044759500026702925, 0.21142616669336955, 0.06774049997329712, 0.18225950002670288, 0.4947595000267029, 0.03649049997329712, 0.03784919562547101, 0.005240499973297119, 0.03649049997329712, 0.038877147085526365, 0.0719071666399638, 0.02309764283043997, 0.0052405595779418945, 0.09475950002670286, 0.005240529775619507, 0.11142625610033674, 0.14809764283043997, 0.02600950002670288, 0.04163450002670288, 0.16663450002670288, 0.03301827775107491, 0.1052404999732971, 0.0052404701709747314, 0.11142616669336958, 0.09091337598287136, 0.04370203843483553, 0.05725950002670288, 0.05524049997329711, 0.3161880714552743, 0.2552404999732971, 0.09274049997329714, 0.09899047017097473, 0.11975950002670288, 0.08299479414434996, 0.03649047017097473, 0.1572596490383148, 0.007917394763544972, 0.07809289296468103, 0.08857383330663043, 0.04690716663996383, 0.026009559631347656, 0.27600958943367004, 0.018569023836226717, 0.1447595000267029, 0.09452621425901142, 0.005240499973297119, 0.2832210384882413, 0.04095478568758282, 0.04935814703212063, 0.01975955963134768, 0.23389005401860113, 0.17123008826199704, 0.16142616669336946, 0.14412929945521885, 0.13024049997329712, 0.25725958943367, 0.005240499973297119, 0.09198172224892509, 0.2760095000267029, 0.03301827775107491, 0.038877147085526365, 0.15100950002670288, 0.13024047017097473, 0.02190716663996381, 0.1447595000267029, 0.20904521431241718, 0.21975950002670286, 0.024171264732585296, 0.17657768184488465, 0.05787207892066551, 0.049358087427475855, 0.044759500026702925, 0.05211552977561951, 0.03649047017097473, 0.01839839471013921, 0.02600950002670288, 0.038573833306630445, 0.14181832355611468, 0.16595478568758282, 0.09899049997329712, 0.05787207892066551, 0.03225950002670286, 0.07666907140186857, 0.005240499973297119, 0.04475952982902531, 0.03423318423722921, 0.018569023836226717, 0.024171264732585296, 0.03423318423722921, 0.01912932925754124, 0.10818167644388538, 0.02086549997329712, 0.20166907140186857, 0.02600950002670288, 0.007917394763544972, 0.0535830294384676, 0.2603845000267029, 0.005240499973297119, 0.10839604247700091, 0.03642616669336951, 0.15524049997329714, 0.04095478568758282, 0.13985588458868176, 0.10839586366306658, 0.17809283336003623, 0.05211549997329712, 0.024171264732585296, 0.07468494441774151, 0.11975950002670288, 0.038877147085526365, 0.019129388862186014, 0.028092833360036207, 0.22913450002670288, 0.1123832677091871, 0.11975950002670288, 0.08999759526479811, 0.19714051201229993, 0.04237863705271777, 0.01714523207573662, 0.01975955963134768, 0.03649049997329712, 0.005240499973297119, 0.03642616669336951, 0.01038450002670288, 0.3697595000267029, 0.05211549997329712, 0.3228845000267029, 0.02447126920406631, 0.038573833306630445, 0.08299479414434996, 0.02309764283043997, 0.044759500026702925, 0.11240655885023232, 0.1947595000267029, 0.17531505558225846, 0.055240529775619496, 0.04095478568758282, 0.038573833306630445, 0.11240655885023232, 0.009465382379644005, 0.07430495457215747, 0.15524049997329714, 0.0642039444711473, 0.02086549997329712, 0.0919816924466027, 0.019129388862186014, 0.05881192854472567, 0.04163450002670288, 0.1447595000267029, 0.18857383330663047, 0.03423318423722921, 0.005240499973297119, 0.2603845000267029, 0.03423318423722921, 0.1052404999732971, 0.00864838891559172, 0.17333092859813148, 0.02086549997329712, 0.0535830294384676, 0.10246272219551933, 0.03649049997329712, 0.007917394763544972, 0.07876991173800302, 0.08857383330663043, 0.05881192854472567, 0.006664261931464743, 0.15100950002670288, 0.0535830294384676, 0.04935814703212063, 0.10413450002670288, 0.04739104917174897, 0.05524049997329711, 0.02253727780448067, 0.06774049997329712, 0.2169817222489251, 0.10818167644388538, 0.0719071666399638, 0.11975950002670288, 0.11238335711615427, 0.05881192854472567, 0.028092833360036207, 0.42123008826199704, 0.019946382326238327, 0.10839586366306658, 0.08857383330663043, 0.19118807145527428, 0.1802404999732971, 0.06828891179140872, 0.11975950002670288, 0.15107383330663043, 0.1197594702243805, 0.2780928035577138, 0.019129388862186014, 0.07809283336003625, 0.005240499973297119, 0.019759470224380515, 0.04684283336003625, 0.019129388862186014, 0.11318055265828186, 0.07666907140186857, 0.1719071666399638, 0.044759500026702925, 0.021907196442286198, 0.04935814703212063, 0.09899049997329712, 0.04935817683444302, 0.31975950002670284, 0.03301827775107491, 0.06054897371091339, 0.03465226467917948, 0.10190235716955998, 0.0719071666399638, 0.02086549997329712, 0.06975950002670289, 0.024171264732585296, 0.1058706111378141, 0.18857383330663047, 0.0330183075533973, 0.11240655885023232, 0.06975950002670289, 0.02086549997329712, 0.2308706111378141, 0.1565242059090558, 0.005240499973297119, 0.005240499973297119, 0.32809283336003625, 0.021075289500387062, 0.08850950002670288, 0.012616642883845786, 0.03465226467917948, 0.02253727780448067, 0.16142616669336946, 0.0114261666933696, 0.21142616669336955, 0.04095478568758282, 0.005240529775619507, 0.01774049997329713, 0.005240499973297119, 0.13759344114976768, 0.08336549997329712, 0.10190235716955998, 0.03301827775107491, 0.03047378574098858, 0.20524049997329713, 0.2006418529678794, 0.05787207892066551, 0.01038450002670288, 0.03301827775107491, 0.1802404999732971, 0.16663450002670288, 0.1197594702243805, 0.08225947022438052, 0.012616642883845786, 0.044759500026702925, 0.0642039444711473, 0.11240655885023232, 0.03301827775107491, 0.22746272219551933, 0.24475950002670288, 0.13024049997329712, 0.1526542368688082, 0.05725950002670288, 0.08336549997329712, 0.2006418529678794, 0.06406402938506184, 0.0847595000267029, 0.02086549997329712, 0.09475950002670286, 0.06079605552885273, 0.38225950002670284, 0.005240499973297119, 0.07168257694977981, 0.08299479414434996, 0.13024049997329712, 0.0977006466949687, 0.019946382326238327, 0.08299476434202757, 0.005240499973297119, 0.01774049997329713, 0.19788450002670288, 0.02600950002670288, 0.13024049997329712, 0.0719071666399638, 0.09198172224892509, 0.07809283336003625, 0.13024049997329712, 0.005240529775619507, 0.07102997365750763, 0.0114261666933696, 0.005240499973297119, 0.1565242059090558, 0.09475950002670286, 0.07288450002670288, 0.09899049997329712, 0.06142616669336953, 0.044759500026702925, 0.4947595000267029, 0.05881192854472567, 0.07288450002670288, 0.24475950002670288, 0.09734576313119186, 0.16975950002670287, 0.009465382379644005, 0.1921279210793344, 0.11142616669336958, 0.2697595000267029, 0.06079605552885273, 0.15524049997329714, 0.24475950002670288, 0.11635161108440822, 0.18225950002670288, 0.259465382379644, 0.06618807145527428, 0.20904521431241718, 0.08850950002670288, 0.10725950002670287, 0.09899049997329712, 0.009465382379644005, 0.12711244120317344, 0.038573833306630445, 0.024171264732585296, 0.19118807145527428, 0.0642039444711473, 0.09770067649729108, 0.2308706111378141, 0.14181832355611468, 0.05787207892066551, 0.005240499973297119, 0.02086549997329712, 0.09475950002670286, 0.02086549997329712, 0.46975950002670286, 0.06774049997329712, 0.005240499973297119, 0.08999759526479811, 0.006123136390339234, 0.24475950002670288, 0.09475950002670286, 0.07876991173800302, 0.06975950002670289, 0.03301827775107491, 0.044759500026702925, 0.2603845000267029, 0.08299479414434996, 0.38225950002670284, 0.34475950002670286, 0.4478845000267029, 0.10246272219551933, 0.0535830294384676, 0.05881192854472567, 0.259465382379644, 0.05211549997329712, 0.038573833306630445, 0.006664232129142356, 0.2169817222489251, 0.09198172224892509, 0.03642616669336951, 0.04163450002670288, 0.06774049997329712, 0.08299479414434996, 0.11240655885023232, 0.0114261666933696, 0.030240499973297114, 0.3399975952647981, 0.20524049997329713, 0.2780928333600362, 0.24475950002670288, 0.024171264732585296, 0.1565242059090558, 0.019129388862186014, 0.08418786839434977, 0.06206719233439517, 0.2614261666933696, 0.4114261666933695, 0.0728844702243805, 0.11461549997329712, 0.005240499973297119, 0.047391078974071355, 0.08336549997329712, 0.03642616669336951, 0.09899049997329712, 0.13364838891559166, 0.11142616669336958, 0.1565242059090558, 0.06054897371091339, 0.15229932350270886, 0.01714526187805901, 0.24475950002670288, 0.02309764283043997, 0.16663450002670288, 0.03047378574098858, 0.05211549997329712, 0.07876991173800302, 0.06618807145527428, 0.04163450002670288, 0.019759500026702903, 0.15100950002670288, 0.23005361767376176, 0.01839839471013921, 0.05031505558225846, 0.11975950002670288, 0.0642039444711473, 0.005240499973297119, 0.07370683864543315, 0.01038450002670288, 0.07809283336003625, 0.15524049997329714, 0.200641823165557, 0.11238335711615427, 0.0746849742200639, 0.23160160528986068, 0.05285954759234471, 0.048330928598131484, 0.03465226467917948, 0.06975950002670289, 0.13538450002670288, 0.33686476318459757, 0.04935814703212063, 0.2916344702243805, 0.07102997365750763, 0.07666907140186857, 0.14475947022438052, 0.06618804165295189, 0.14809764283043997, 0.31142613689104715, 0.06079605552885273, 0.005240499973297119, 0.05881192854472567, 0.005240499973297119, 0.4392039444711473, 0.0114261666933696, 0.009465382379644005, 0.040214045481248295, 0.08478595451875165, 0.09452621425901142, 0.21350950002670288, 0.1058706111378141, 0.2804737857409886, 0.15524049997329714, 0.05725950002670288, 0.017486772753975588, 0.06774049997329712, 0.1058706111378141, 0.08999759526479811, 0.0642039444711473, 0.21142616669336955, 0.07102997365750763, 0.019129388862186014, 0.08857383330663043, 0.005240499973297119, 0.1978844702243805, 0.044759500026702925, 0.04163450002670288, 0.005240499973297119, 0.06294131820852111, 0.005240499973297119, 0.02086549997329712, 0.05725950002670288, 0.05725947022438049, 0.022537248002158283, 0.12288755879682656, 0.048330928598131484, 0.061426136891047145, 0.019946382326238327, 0.03301827775107491, 0.08850950002670288, 0.05211549997329712, 0.07809283336003625, 0.08299479414434996, 0.20524049997329713, 0.019129388862186014, 0.054283309550512415, 0.038573833306630445, 0.03225950002670286, 0.24475950002670288, 0.11142616669336958, 0.03465226467917948, 0.03225950002670286, 0.09703222729943017, 0.09475950002670286, 0.09452624406133381, 0.038877147085526365, 0.06774049997329712, 0.04935814703212063, 0.20524049997329713, 0.17123008826199704, 0.01839839471013921, 0.03465226467917948, 0.005240529775619507, 0.005240499973297119, 0.07809283336003625, 0.03642616669336951, 0.10190235716955998, 0.007917394763544972, 0.019759470224380515, 0.4947595000267029, 0.02086549997329712, 0.5225372778044806, 0.17531505558225846, 0.03423318423722921, 0.06142616669336953, 0.03423318423722921, 0.06079605552885273, 0.01839839471013921, 0.10413450002670288, 0.02309764283043997, 0.005240499973297119, 0.08024049997329713, 0.10246272219551933, 0.06406402938506184, 0.05031505558225846, 0.02309764283043997, 0.08404521431241718, 0.0719071666399638, 0.02253727780448067, 0.029050023782820983, 0.25791736496122264, 0.2725372480021583, 0.2804737857409886, 0.09258558698322455, 0.0728844702243805, 0.006664261931464743, 0.04471418418382345, 0.07666907140186857, 0.005240499973297119, 0.04095478568758282, 0.1802405297756195, 0.005240499973297119, 0.03301827775107491, 0.06774049997329712, 0.17333092859813148, 0.04471418418382345, 0.13024049997329712, 0.11238335711615427, 0.11461549997329712, 0.03465226467917948, 0.212150804374529, 0.02253727780448067, 0.05285954759234471, 0.12809283336003618, 0.07288450002670288, 0.02190716663996381, 0.0977006466949687, 0.005240499973297119, 0.28887714708552636, 0.05725950002670288, 0.023097672632762356, 0.005240499973297119, 0.27253727780448067, 0.02253727780448067, 0.05031505558225846, 0.0719071666399638, 0.03047378574098858, 0.1052404999732971, 0.08686476318459757, 0.02447126920406631, 0.0642039444711473, 0.06054897371091339, 0.08404521431241718, 0.257917394763545, 0.005240499973297119, 0.15100950002670288, 0.012616642883845786, 0.31142616669336953, 0.1360638180504674, 0.042740499973297125, 0.09091334618054897, 0.09899049997329712, 0.10818167644388538, 0.15100950002670288, 0.040214045481248295, 0.03857386310895283, 0.03047378574098858, 0.06205868179147897, 0.0747595000267029, 0.09475950002670286, 0.03465226467917948, 0.18225950002670288, 0.05725950002670288, 0.03857386310895283, 0.1719071666399638, 0.05524049997329711, 0.1947595000267029, 0.13759344114976768, 0.08024049997329713, 0.021075289500387062, 0.05524049997329711, 0.08024049997329713, 0.005240499973297119, 0.059976891331050664, 0.028092833360036207, 0.12190716663996376, 0.1197594702243805, 0.11635161108440822, 0.07666907140186857, 0.005240529775619507, 0.028092833360036207, 0.2902140454812483, 0.04163450002670288, 0.3142039146688249, 0.06618807145527428, 0.03649049997329712, 0.12711244120317344, 0.08299479414434996, 0.06774049997329712, 0.040214045481248295, 0.15524049997329714, 0.22746272219551933, 0.02309764283043997, 0.030240499973297114, 0.08024049997329713, 0.05787207892066551, 0.06774049997329712, 0.005240499973297119, 0.005240499973297119, 0.06975950002670289, 0.005240499973297119, 0.054283309550512415, 0.012616642883845786, 0.0719071666399638, 0.06828891179140872, 0.038573833306630445, 0.05725950002670288, 0.06054897371091339, 0.04163450002670288, 0.01774049997329713, 0.1892039444711473, 0.05031505558225846, 0.01038450002670288, 0.01839839471013921, 0.0719071666399638, 0.06828891179140872, 0.1058706111378141, 0.17123008826199704, 0.03465226467917948, 0.04690716663996383, 0.3876166428838458, 0.07468494441774151, 0.02190716663996381, 0.03465226467917948, 0.15100950002670288, 0.12711244120317344, 0.08857383330663043, 0.05211549997329712, 0.011426136891047212, 0.06142616669336953, 0.13857383330663048, 0.3035829996361452, 0.09452621425901142, 0.0934757940909442, 0.1052404999732971, 0.05524049997329711, 0.01774049997329713, 0.10839586366306658, 0.03649049997329712, 0.10413450002670288, 0.01038450002670288, 0.13761664288384579, 0.005240499973297119, 0.32809283336003625, 0.2983309285981315, 0.04935814703212063, 0.2603845000267029, 0.10190235716955998, 0.040954815489905205, 0.05031505558225846, 0.07809283336003625, 0.03301827775107491, 0.07876991173800302, 0.0822595000267029, 0.13538450002670288, 0.05881192854472567, 0.1058706111378141, 0.1058706111378141, 0.3340452143124172, 0.2552404999732971, 0.034652294481501866, 0.044759500026702925, 0.08566859093579376, 0.005240499973297119, 0.06774049997329712, 0.03225950002670286, 0.021907196442286198, 0.10818170624620776, 0.27253727780448067, 0.02190716663996381, 0.03423318423722921, 0.10246272219551933, 0.1058706111378141, 0.09899049997329712, 0.06406402938506184, 0.08336549997329712, 0.05524049997329711, 0.005240499973297119, 0.012616642883845786, 0.05524049997329711, 0.030240499973297114, 0.031556289446981356, 0.28887714708552636, 0.06293280766560483, 0.005240499973297119, 0.02086549997329712, 0.05245180771901059, 0.07809283336003625, 0.005240499973297119, 0.13024049997329712, 0.16595478568758282, 0.12190716663996376, 0.038573833306630445, 0.0535830294384676, 0.1322595000267029, 0.14412938886218601, 0.16663450002670288, 0.048330928598131484, 0.04935814703212063, 0.4197595000267029, 0.1052404999732971, 0.11975950002670288, 0.33299479414434996, 0.08024049997329713, 0.3697595000267029, 0.019946382326238327, 0.06206719233439517, 0.16142616669336946, 0.2991073261136594, 0.005240499973297119, 0.212150804374529, 0.05157768184488465, 0.14181832355611468, 0.12711241140085106, 0.20524049997329713, 0.007259500026702892, 0.06774049997329712, 0.1802404999732971, 0.01774049997329713, 0.07809280355771386, 0.031556289446981356, 0.019129388862186014, 0.005240499973297119, 0.03642616669336951, 0.021075289500387062, 0.1947595000267029, 0.2780928333600362, 0.11975950002670288, 0.005240529775619507, 0.005240499973297119, 0.012616642883845786, 0.22809283336003616, 0.22913450002670288, 0.08299479414434996, 0.22809283336003616, 0.06142616669336953, 0.09198172224892509, 0.09770067649729108, 0.016604136336933473, 0.05881192854472567, 0.02600950002670288, 0.038877147085526365, 0.05524049997329711, 0.04237854764575061, 0.005240499973297119, 0.11975950002670288, 0.024171264732585296, 0.08299479414434996, 0.019129388862186014, 0.21142616669336955, 0.05031505558225846, 0.2760095000267029, 0.06774049997329712, 0.1058706111378141, 0.13024049997329712, 0.05524049997329711, 0.05524049997329711, 0.009465382379644005, 0.0934757940909442, 0.05031505558225846, 0.31975950002670284, 0.3072595000267029, 0.07370686844775554, 0.05881192854472567, 0.21142616669336955, 0.03465226467917948, 0.06406402938506184, 0.03649049997329712, 0.06774049997329712, 0.019129388862186014, 0.006123136390339234, 0.0719071666399638, 0.09475947022438047, 0.11317909234448487, 0.13024049997329712, 0.01038450002670288, 0.05358299963614521, 0.05031505558225846, 0.07876991173800302, 0.09475950002670286, 0.06142616669336953, 0.07809283336003625, 0.028092833360036207, 0.005240499973297119, 0.06828879258211917, 0.05725947022438049, 0.2760095000267029, 0.08418786839434977, 0.01038450002670288, 0.0719071666399638, 0.028092833360036207, 0.038573833306630445, 0.3161880714552743, 0.16142616669336946, 0.02600950002670288, 0.15385040911761194, 0.00864838891559172, 0.14753727780448067, 0.015592833360036251, 0.005240499973297119, 0.01839839471013921, 0.14181832355611468, 0.011426136891047212, 0.2626166428838458, 0.005240499973297119, 0.005240499973297119, 0.10524052977561948, 0.3601441154113183, 0.048330928598131484, 0.08024049997329713, 0.07468494441774151, 0.0719071666399638, 0.4669817222489251, 0.05725950002670288, 0.04163450002670288, 0.06142616669336953, 0.20904521431241718, 0.06828891179140872, 0.03649049997329712, 0.2916345000267029, 0.09452621425901142, 0.2697594702243805, 0.05211549997329712, 0.13017616669336946, 0.2552404999732971, 0.08299479414434996, 0.040954815489905205, 0.08850950002670288, 0.038573833306630445, 0.047391078974071355, 0.05725947022438049, 0.04471418418382345, 0.1802404999732971, 0.2447594702243805, 0.0330183075533973, 0.1526542070664858, 0.08171602176583337, 0.47690235716956, 0.13024049997329712, 0.1947595000267029, 0.1947594702243805, 0.09475950002670286, 0.038573833306630445, 0.022537248002158283, 0.0535830294384676, 0.09452624406133381, 0.10190235716955998, 0.022537218199835896, 0.05524049997329711, 0.2169817222489251, 0.05031505558225846, 0.0677405297756195, 0.019759500026702903, 0.21350950002670288, 0.13538450002670288, 0.05787207892066551, 0.3182889117914087, 0.35587061113781404, 0.0934757940909442, 0.1947595000267029, 0.042740499973297125, 0.12809283336003618, 0.021075289500387062, 0.06142616669336953, 0.08336549997329712, 0.07666907140186857, 0.18593597061493816, 0.05787207892066551, 0.019129388862186014, 0.13024049997329712, 0.08850950002670288, 0.23005361767376176, 0.04095478568758282, 0.03465226467917948, 0.07666907140186857, 0.012616613081523398]\n",
            "Test loss: 0.097\n",
            "Test accuracy: 35.769%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anfDvf0K_Te6",
        "outputId": "1c72a9e1-9247-4cf9-805b-c0378bf62ee1"
      },
      "source": [
        "print(\"THE END\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "THE END\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}