{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS505FinalProject_final_version.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn7wB6kn_p5g"
      },
      "source": [
        "# By Mingcheng Xu\n",
        "# This notebook contains 5 models:\n",
        "\n",
        "1. Linear regression with word length, word frequency and pre-trained Glove embeddings\n",
        "\n",
        "2. LSTM classification\n",
        "\n",
        "3. LSTM classification with Glove\n",
        "\n",
        "4. LSTM that predicts complexity as a float number\n",
        "\n",
        "5. LSTM that predicts a float number, with Glove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK0gMeXPKEEX"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from copy import deepcopy\n",
        "import math\n",
        "\n",
        "import torchtext.vocab\n",
        "\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYtWoHTOKcNk",
        "outputId": "5aaa1aac-33a1-4fd9-9fca-cc0ee5b52303"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNkKxGDiKcIN"
      },
      "source": [
        "# the paths; change when necessary\n",
        "TRAIN_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train.tsv\"\n",
        "TEST_RAW = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test.tsv\"\n",
        "TRAIN = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_train_cleaned.tsv\"\n",
        "TEST = \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/lcp_single_test_cleaned.tsv\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVQ9mky8KcFb"
      },
      "source": [
        "# read the datasets\n",
        "# train\n",
        "with open(TRAIN_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "# need to remove \" from the string, otherwise parsing will have problems because some quotas are not closed \n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TRAIN, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "df = pd.read_csv(TRAIN, sep='\\t')\n",
        "\n",
        "# test\n",
        "with open(TEST_RAW, 'r') as f:\n",
        "  data = f.read()\n",
        "\n",
        "data = data.replace('\"', '')\n",
        "\n",
        "with open(TEST, 'w') as f:\n",
        "  f.write(data)\n",
        "\n",
        "test = pd.read_csv(TEST, sep='\\t')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "Vw8Gk02zKcCc",
        "outputId": "2675e9d2-20bd-4530-e834-25a7752eb82e"
      },
      "source": [
        "# take a look\n",
        "pd.set_option('display.max_colwidth', None) # show the whole sentence\n",
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3ZLW647WALVGE8EBR50EGUBPU4P32A</td>\n",
              "      <td>bible</td>\n",
              "      <td>Behold, there came up out of the river seven cattle, sleek and fat, and they fed in the marsh grass.</td>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34R0BODSP1ZBN3DVY8J8XSIY551E5C</td>\n",
              "      <td>bible</td>\n",
              "      <td>I am a fellow bondservant with you and with your brothers, the prophets, and with those who keep the words of this book.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3S1WOPCJFGTJU2SGNAN2Y213N6WJE3</td>\n",
              "      <td>bible</td>\n",
              "      <td>The man, the lord of the land, said to us, 'By this I will know that you are honest men: leave one of your brothers with me, and take grain for the famine of your houses, and go your way.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFNCI9LYKQN09BHXHH9CLSX5KP738</td>\n",
              "      <td>bible</td>\n",
              "      <td>Shimei had sixteen sons and six daughters; but his brothers didn't have many children, neither did all their family multiply like the children of Judah.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2</td>\n",
              "      <td>bible</td>\n",
              "      <td>He has put my brothers far from me.</td>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3ZLW647WALVGE8EBR50EGUBPU4P32A  bible  ...     river   0.000000\n",
              "1  34R0BODSP1ZBN3DVY8J8XSIY551E5C  bible  ...  brothers   0.000000\n",
              "2  3S1WOPCJFGTJU2SGNAN2Y213N6WJE3  bible  ...  brothers   0.050000\n",
              "3  3BFNCI9LYKQN09BHXHH9CLSX5KP738  bible  ...  brothers   0.150000\n",
              "4  3G5RUKN2EC3YIWSKUXZ8ZVH95R49N2  bible  ...  brothers   0.263889\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "UTLwl2AWa-XG",
        "outputId": "ea24f622-6e23-45bf-b2da-bc30c134f9b9"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>corpus</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3K8CQCU3KE19US5SN890DFPK3SANWR</td>\n",
              "      <td>bible</td>\n",
              "      <td>But he, beckoning to them with his hand to be silent, declared to them how the Lord had brought him out of the prison.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3Q2T3FD0ON86LCI41NJYV3PN0BW3MV</td>\n",
              "      <td>bible</td>\n",
              "      <td>If I forget you, Jerusalem, let my right hand forget its skill.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B</td>\n",
              "      <td>bible</td>\n",
              "      <td>the ten sons of Haman the son of Hammedatha, the Jew's enemy, but they didn't lay their hand on the plunder.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD</td>\n",
              "      <td>bible</td>\n",
              "      <td>Let your hand be lifted up above your adversaries, and let all of your enemies be cut off.</td>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3QREJ3J433XSBS8QMHAICCR0BQ1LKR</td>\n",
              "      <td>bible</td>\n",
              "      <td>Abimelech chased him, and he fled before him, and many fell wounded, even to the entrance of the gate.</td>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                               id corpus  ...     token complexity\n",
              "0  3K8CQCU3KE19US5SN890DFPK3SANWR  bible  ...      hand   0.000000\n",
              "1  3Q2T3FD0ON86LCI41NJYV3PN0BW3MV  bible  ...      hand   0.197368\n",
              "2  3ULIZ0H1VA5C32JJMKOTQ8Z4GUS51B  bible  ...      hand   0.200000\n",
              "3  3BFF0DJK8XCEIOT30ZLBPPSRMZQTSD  bible  ...      hand   0.267857\n",
              "4  3QREJ3J433XSBS8QMHAICCR0BQ1LKR  bible  ...  entrance   0.000000\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "NsykAZ1Zi6ti",
        "outputId": "05e6336c-75ae-4109-c74a-4719f354aa6a"
      },
      "source": [
        "# the distribution of the complexities in training set\n",
        "look_complexities = df['complexity'].to_list()\n",
        "x_axis = [i for i in range(len(look_complexities))]\n",
        "plt.scatter(x_axis, look_complexities)\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dN3ToWiZQlyk"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPdQRWvvaQyX"
      },
      "source": [
        "#Try linear regression first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYqivG31Lubb"
      },
      "source": [
        "def create_weights_matrix(vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in Glove \"\"\"\n",
        "  matrix_len = len(vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension))\n",
        "\n",
        "  for i, word in enumerate(vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = glove[word]\n",
        "      except KeyError:\n",
        "          weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, )) # initialize a random vector\n",
        "  #return torch.from_numpy(weights_matrix) # a tensor\n",
        "  return weights_matrix"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhOsDaEgacAF"
      },
      "source": [
        "# use the Glove 6B 100d\n",
        "cache_dir = \"/content/gdrive/My Drive/Colab Notebooks/data\"\n",
        "# glove = vocab.pretrained_aliases[\"glove.6B.100d\"](cache=cache_dir)\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100, cache=cache_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgCVuZWyZEXm",
        "outputId": "111de77c-02db-4c5c-c164-dd4de03926f5"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0x_EbtYiab9v",
        "outputId": "6517f0e3-bdc2-4943-fbb8-088fb4f3a96e"
      },
      "source": [
        "# get all the non-unique tokens for prediction\n",
        "tokens = df['token'].dropna().to_list()\n",
        "tokens = [token.lower() for token in tokens] # lowercase\n",
        "print(len(tokens))\n",
        "\n",
        "# check if all tokens are in Glove\n",
        "for token in tokens:\n",
        "  if token not in glove.stoi:\n",
        "    print(\"Token Not Found: \")\n",
        "    print(token)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7659\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "perverseness\n",
            "Token Not Found: \n",
            "housetops\n",
            "Token Not Found: \n",
            "slanderers\n",
            "Token Not Found: \n",
            "plowmen\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dainties\n",
            "Token Not Found: \n",
            "dunghill\n",
            "Token Not Found: \n",
            "carotids\n",
            "Token Not Found: \n",
            "tace\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "aaZuP5QmeGgf",
        "outputId": "acfc06c8-c98f-4ef8-9e9c-4ddf35cbdcb4"
      },
      "source": [
        "# create a dataframe for linear regression\n",
        "train_df = pd.DataFrame(tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "train_df['complexity'] = df['complexity']\n",
        "\n",
        "# word length\n",
        "train_df['word_length'] = train_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# punctuations\n",
        "punc = string.punctuation\n",
        "\n",
        "# stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = df['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count_word_frequency = Counter(texts)\n",
        "train_df['word_frequency'] = train_df['token'].map(lambda x: count_word_frequency[x])\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  word_frequency\n",
              "0     river    0.000000            5              26\n",
              "1  brothers    0.000000            8              36\n",
              "2  brothers    0.050000            8              36\n",
              "3  brothers    0.150000            8              36\n",
              "4  brothers    0.263889            8              36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "KU-Kz_ThLuYD",
        "outputId": "0a132508-e0d9-4850-da9b-207f6734abc7"
      },
      "source": [
        "# create the weight matrix\n",
        "weights_matrix = create_weights_matrix(tokens)\n",
        "print(weights_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weights_matrix_df = pd.DataFrame(weights_matrix)\n",
        "\n",
        "train_df_combined = pd.concat([train_df, weights_matrix_df], axis=1)\n",
        "train_df_combined.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7659, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>26</td>\n",
              "      <td>-0.33249</td>\n",
              "      <td>-0.56631</td>\n",
              "      <td>0.54255</td>\n",
              "      <td>-0.11869</td>\n",
              "      <td>0.531290</td>\n",
              "      <td>-0.49381</td>\n",
              "      <td>0.64114</td>\n",
              "      <td>0.85982</td>\n",
              "      <td>0.39633</td>\n",
              "      <td>-1.53950</td>\n",
              "      <td>-0.30613</td>\n",
              "      <td>0.97267</td>\n",
              "      <td>-0.31192</td>\n",
              "      <td>-0.10311</td>\n",
              "      <td>0.359510</td>\n",
              "      <td>-0.60023</td>\n",
              "      <td>0.909830</td>\n",
              "      <td>-0.959540</td>\n",
              "      <td>-0.55375</td>\n",
              "      <td>0.082818</td>\n",
              "      <td>0.26711</td>\n",
              "      <td>0.64645</td>\n",
              "      <td>-0.098556</td>\n",
              "      <td>0.539240</td>\n",
              "      <td>-0.21810</td>\n",
              "      <td>-0.13430</td>\n",
              "      <td>-1.80700</td>\n",
              "      <td>-0.14879</td>\n",
              "      <td>0.39006</td>\n",
              "      <td>-0.62883</td>\n",
              "      <td>-0.38825</td>\n",
              "      <td>0.31925</td>\n",
              "      <td>0.77853</td>\n",
              "      <td>-0.60273</td>\n",
              "      <td>0.063585</td>\n",
              "      <td>-0.75916</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.53185</td>\n",
              "      <td>0.72585</td>\n",
              "      <td>0.36811</td>\n",
              "      <td>0.19494</td>\n",
              "      <td>0.64276</td>\n",
              "      <td>0.81460</td>\n",
              "      <td>0.26748</td>\n",
              "      <td>-0.39275</td>\n",
              "      <td>0.425950</td>\n",
              "      <td>0.11699</td>\n",
              "      <td>0.21063</td>\n",
              "      <td>-0.061747</td>\n",
              "      <td>0.79298</td>\n",
              "      <td>-0.45978</td>\n",
              "      <td>0.85176</td>\n",
              "      <td>-0.36726</td>\n",
              "      <td>0.11816</td>\n",
              "      <td>0.504160</td>\n",
              "      <td>-0.065352</td>\n",
              "      <td>0.69672</td>\n",
              "      <td>0.37525</td>\n",
              "      <td>0.92586</td>\n",
              "      <td>-0.83036</td>\n",
              "      <td>-0.087948</td>\n",
              "      <td>-0.49715</td>\n",
              "      <td>0.21411</td>\n",
              "      <td>-0.82838</td>\n",
              "      <td>-0.85912</td>\n",
              "      <td>0.61576</td>\n",
              "      <td>1.18800</td>\n",
              "      <td>-0.30745</td>\n",
              "      <td>-1.20090</td>\n",
              "      <td>-1.70970</td>\n",
              "      <td>0.51400</td>\n",
              "      <td>-1.01590</td>\n",
              "      <td>0.55555</td>\n",
              "      <td>-1.03850</td>\n",
              "      <td>-0.69940</td>\n",
              "      <td>1.050600</td>\n",
              "      <td>0.24051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>8</td>\n",
              "      <td>36</td>\n",
              "      <td>0.85968</td>\n",
              "      <td>-0.39038</td>\n",
              "      <td>-0.62678</td>\n",
              "      <td>-0.55279</td>\n",
              "      <td>0.097012</td>\n",
              "      <td>0.00658</td>\n",
              "      <td>-0.65021</td>\n",
              "      <td>-0.58272</td>\n",
              "      <td>-1.27630</td>\n",
              "      <td>0.11251</td>\n",
              "      <td>0.78504</td>\n",
              "      <td>0.16027</td>\n",
              "      <td>0.38327</td>\n",
              "      <td>0.62672</td>\n",
              "      <td>-0.017462</td>\n",
              "      <td>-0.36443</td>\n",
              "      <td>0.062441</td>\n",
              "      <td>0.039266</td>\n",
              "      <td>-0.47318</td>\n",
              "      <td>0.547680</td>\n",
              "      <td>0.42916</td>\n",
              "      <td>-0.25516</td>\n",
              "      <td>0.100900</td>\n",
              "      <td>0.041618</td>\n",
              "      <td>-0.14579</td>\n",
              "      <td>0.15174</td>\n",
              "      <td>-0.54301</td>\n",
              "      <td>-0.29787</td>\n",
              "      <td>0.36268</td>\n",
              "      <td>0.89550</td>\n",
              "      <td>0.65319</td>\n",
              "      <td>0.40141</td>\n",
              "      <td>0.03668</td>\n",
              "      <td>-0.34313</td>\n",
              "      <td>-0.102040</td>\n",
              "      <td>-0.19383</td>\n",
              "      <td>...</td>\n",
              "      <td>0.65085</td>\n",
              "      <td>0.97366</td>\n",
              "      <td>0.36997</td>\n",
              "      <td>-0.58266</td>\n",
              "      <td>0.58389</td>\n",
              "      <td>-0.62574</td>\n",
              "      <td>-0.24252</td>\n",
              "      <td>1.37500</td>\n",
              "      <td>-0.042651</td>\n",
              "      <td>0.16398</td>\n",
              "      <td>-0.53462</td>\n",
              "      <td>0.552750</td>\n",
              "      <td>-0.58019</td>\n",
              "      <td>-0.78386</td>\n",
              "      <td>-0.18787</td>\n",
              "      <td>-0.20305</td>\n",
              "      <td>0.11506</td>\n",
              "      <td>-0.089296</td>\n",
              "      <td>-0.766080</td>\n",
              "      <td>0.04339</td>\n",
              "      <td>0.50251</td>\n",
              "      <td>0.73799</td>\n",
              "      <td>0.23388</td>\n",
              "      <td>0.200380</td>\n",
              "      <td>-0.93906</td>\n",
              "      <td>-0.33974</td>\n",
              "      <td>-0.56534</td>\n",
              "      <td>-0.95945</td>\n",
              "      <td>-0.14597</td>\n",
              "      <td>-0.35173</td>\n",
              "      <td>-0.40463</td>\n",
              "      <td>-0.32671</td>\n",
              "      <td>0.24982</td>\n",
              "      <td>-0.27804</td>\n",
              "      <td>-0.99877</td>\n",
              "      <td>-0.39367</td>\n",
              "      <td>-0.30087</td>\n",
              "      <td>-0.24623</td>\n",
              "      <td>0.006483</td>\n",
              "      <td>-0.21982</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...       97        98       99\n",
              "0     river    0.000000            5  ... -0.69940  1.050600  0.24051\n",
              "1  brothers    0.000000            8  ... -0.24623  0.006483 -0.21982\n",
              "2  brothers    0.050000            8  ... -0.24623  0.006483 -0.21982\n",
              "3  brothers    0.150000            8  ... -0.24623  0.006483 -0.21982\n",
              "4  brothers    0.263889            8  ... -0.24623  0.006483 -0.21982\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNNFO_uLuVz"
      },
      "source": [
        "# get data for training\n",
        "X_train = train_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_train = train_df_combined['complexity']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzmE0l_emB6S"
      },
      "source": [
        "# train linear regression\n",
        "lr = LinearRegression().fit(X_train, Y_train)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luTw7T0HmB4B"
      },
      "source": [
        "# predict\n",
        "Y_pred = lr.predict(X_train)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHqtrYN0mB1q",
        "outputId": "27d68cf8-3ccb-4661-f338-30994165745c"
      },
      "source": [
        "# train loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_train[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average training absolute loss is \" + str(abl))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average training absolute loss is 0.07246931733686796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "q98DV01SovTY",
        "outputId": "804a92ee-8137-4b31-cb1b-c3fb679125b6"
      },
      "source": [
        "# on test\n",
        "test_tokens = test['token'].dropna().to_list()\n",
        "test_tokens = [token.lower() for token in test_tokens] # lowercase\n",
        "print(len(test_tokens))\n",
        "\n",
        "# create a dataframe for linear regression\n",
        "test_df = pd.DataFrame(test_tokens, columns =['token'])\n",
        "\n",
        "# add back complexity\n",
        "test_df['complexity'] = test['complexity']\n",
        "\n",
        "# word length\n",
        "test_df['word_length'] = test_df['token'].map(lambda x: len(x))\n",
        "\n",
        "# word frequency\n",
        "# tokenize the whole curpus\n",
        "temp = test['sentence'].to_list()\n",
        "texts = []\n",
        "for sent in temp:\n",
        "  sent = sent.lower()\n",
        "  sent = ''.join([c for c in sent if c not in punc])\n",
        "  words = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  texts += words\n",
        "# count frequency\n",
        "count = Counter(texts)\n",
        "test_df['word_frequency'] = test_df['token'].map(lambda x: count[x])\n",
        "\n",
        "# create the weight matrix\n",
        "weights_matrix = create_weights_matrix(test_tokens)\n",
        "print(weights_matrix.shape)\n",
        "\n",
        "# combine\n",
        "weights_matrix_df = pd.DataFrame(weights_matrix)\n",
        "test_df_combined = pd.concat([test_df, weights_matrix_df], axis=1)\n",
        "test_df_combined.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "917\n",
            "(917, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>word_length</th>\n",
              "      <th>word_frequency</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.197368</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hand</td>\n",
              "      <td>0.267857</td>\n",
              "      <td>4</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.24945</td>\n",
              "      <td>0.37033</td>\n",
              "      <td>-0.058334</td>\n",
              "      <td>-0.25367</td>\n",
              "      <td>0.18709</td>\n",
              "      <td>0.81760</td>\n",
              "      <td>-0.045494</td>\n",
              "      <td>0.072066</td>\n",
              "      <td>-0.059079</td>\n",
              "      <td>-0.053018</td>\n",
              "      <td>-0.15681</td>\n",
              "      <td>-0.18621</td>\n",
              "      <td>0.786770</td>\n",
              "      <td>0.56263</td>\n",
              "      <td>0.023693</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>0.034775</td>\n",
              "      <td>0.117630</td>\n",
              "      <td>-0.15757</td>\n",
              "      <td>-0.39749</td>\n",
              "      <td>0.210680</td>\n",
              "      <td>-0.14618</td>\n",
              "      <td>0.014017</td>\n",
              "      <td>-0.22373</td>\n",
              "      <td>0.54225</td>\n",
              "      <td>0.47379</td>\n",
              "      <td>-0.62683</td>\n",
              "      <td>-0.38803</td>\n",
              "      <td>0.27510</td>\n",
              "      <td>-0.54687</td>\n",
              "      <td>0.49211</td>\n",
              "      <td>0.052715</td>\n",
              "      <td>-0.12911</td>\n",
              "      <td>0.2554</td>\n",
              "      <td>-0.005657</td>\n",
              "      <td>-0.194310</td>\n",
              "      <td>...</td>\n",
              "      <td>0.64518</td>\n",
              "      <td>0.76559</td>\n",
              "      <td>-0.22193</td>\n",
              "      <td>0.39305</td>\n",
              "      <td>0.13373</td>\n",
              "      <td>-0.17641</td>\n",
              "      <td>0.36222</td>\n",
              "      <td>0.47786</td>\n",
              "      <td>-0.43591</td>\n",
              "      <td>-0.13363</td>\n",
              "      <td>-0.13145</td>\n",
              "      <td>0.206730</td>\n",
              "      <td>0.37353</td>\n",
              "      <td>-0.70188</td>\n",
              "      <td>0.53225</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>-0.70940</td>\n",
              "      <td>0.24331</td>\n",
              "      <td>-0.15523</td>\n",
              "      <td>0.20785</td>\n",
              "      <td>1.19970</td>\n",
              "      <td>-0.036297</td>\n",
              "      <td>-0.79044</td>\n",
              "      <td>-0.27794</td>\n",
              "      <td>-1.40760</td>\n",
              "      <td>-0.363180</td>\n",
              "      <td>0.40219</td>\n",
              "      <td>0.17401</td>\n",
              "      <td>-0.080981</td>\n",
              "      <td>-0.40688</td>\n",
              "      <td>-0.044007</td>\n",
              "      <td>-0.14964</td>\n",
              "      <td>0.39369</td>\n",
              "      <td>-0.014732</td>\n",
              "      <td>-0.41309</td>\n",
              "      <td>-0.061931</td>\n",
              "      <td>-0.088387</td>\n",
              "      <td>-0.230930</td>\n",
              "      <td>0.93931</td>\n",
              "      <td>0.091475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>entrance</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>0.25776</td>\n",
              "      <td>0.10680</td>\n",
              "      <td>-0.162650</td>\n",
              "      <td>0.42335</td>\n",
              "      <td>0.19078</td>\n",
              "      <td>0.46283</td>\n",
              "      <td>-0.959150</td>\n",
              "      <td>0.931740</td>\n",
              "      <td>0.471610</td>\n",
              "      <td>0.390770</td>\n",
              "      <td>0.54734</td>\n",
              "      <td>0.41967</td>\n",
              "      <td>0.086822</td>\n",
              "      <td>0.53954</td>\n",
              "      <td>0.354970</td>\n",
              "      <td>-0.028346</td>\n",
              "      <td>0.427080</td>\n",
              "      <td>0.036569</td>\n",
              "      <td>-0.49700</td>\n",
              "      <td>-0.49543</td>\n",
              "      <td>-0.031232</td>\n",
              "      <td>-0.30298</td>\n",
              "      <td>-0.417180</td>\n",
              "      <td>-0.78459</td>\n",
              "      <td>0.70473</td>\n",
              "      <td>-0.59741</td>\n",
              "      <td>-0.33173</td>\n",
              "      <td>-0.38813</td>\n",
              "      <td>0.17189</td>\n",
              "      <td>-0.78565</td>\n",
              "      <td>-0.17219</td>\n",
              "      <td>-0.140190</td>\n",
              "      <td>0.61492</td>\n",
              "      <td>0.5713</td>\n",
              "      <td>0.751090</td>\n",
              "      <td>-0.015942</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.60393</td>\n",
              "      <td>0.47454</td>\n",
              "      <td>0.80912</td>\n",
              "      <td>0.81709</td>\n",
              "      <td>-0.12876</td>\n",
              "      <td>-0.39310</td>\n",
              "      <td>0.17656</td>\n",
              "      <td>-0.29797</td>\n",
              "      <td>-0.32614</td>\n",
              "      <td>-0.26522</td>\n",
              "      <td>-0.37006</td>\n",
              "      <td>-0.016956</td>\n",
              "      <td>0.92268</td>\n",
              "      <td>-0.71606</td>\n",
              "      <td>-0.38524</td>\n",
              "      <td>-0.085737</td>\n",
              "      <td>0.68111</td>\n",
              "      <td>0.32080</td>\n",
              "      <td>0.45870</td>\n",
              "      <td>-0.82737</td>\n",
              "      <td>0.22932</td>\n",
              "      <td>0.314500</td>\n",
              "      <td>-0.21221</td>\n",
              "      <td>-0.65293</td>\n",
              "      <td>-0.31427</td>\n",
              "      <td>-0.037493</td>\n",
              "      <td>0.16126</td>\n",
              "      <td>-0.46719</td>\n",
              "      <td>0.630660</td>\n",
              "      <td>0.26426</td>\n",
              "      <td>0.527780</td>\n",
              "      <td>-0.34505</td>\n",
              "      <td>0.06620</td>\n",
              "      <td>0.722400</td>\n",
              "      <td>-0.11057</td>\n",
              "      <td>-0.005771</td>\n",
              "      <td>-0.059336</td>\n",
              "      <td>0.013272</td>\n",
              "      <td>0.97305</td>\n",
              "      <td>0.454050</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  104 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  complexity  word_length  ...        97       98        99\n",
              "0      hand    0.000000            4  ... -0.230930  0.93931  0.091475\n",
              "1      hand    0.197368            4  ... -0.230930  0.93931  0.091475\n",
              "2      hand    0.200000            4  ... -0.230930  0.93931  0.091475\n",
              "3      hand    0.267857            4  ... -0.230930  0.93931  0.091475\n",
              "4  entrance    0.000000            8  ...  0.013272  0.97305  0.454050\n",
              "\n",
              "[5 rows x 104 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEy_z413ovRJ",
        "outputId": "6660fffd-6775-4b47-f7ea-b93a1f7469fc"
      },
      "source": [
        "# get data for test\n",
        "X_test = test_df_combined.drop(columns=['token', 'complexity'])\n",
        "Y_test = test_df_combined['complexity']\n",
        "\n",
        "# predict\n",
        "Y_pred = lr.predict(X_test)\n",
        "\n",
        "# test loss (average absolute loss)\n",
        "num = len(Y_pred)\n",
        "losses = []\n",
        "for i in range(num):\n",
        "  loss = abs(Y_pred[i] - Y_test[i])\n",
        "  losses.append(loss)\n",
        "abl = sum(losses) / num\n",
        "print(\"average test absolute loss is \" + str(abl))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average test absolute loss is 0.07283375821746224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6xzRuFTovN5"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "un9WFQkFUQcx"
      },
      "source": [
        "#LSTM classification with 5 classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "hDibiYSCLuf1",
        "outputId": "c7d335af-7358-41a6-c90f-92f7ac99db0f"
      },
      "source": [
        "# tokenize sentences\n",
        "\n",
        "def tokenize(sent, token, punc, stop_words):\n",
        "  \"\"\" lowercase, padded, remove stopwords and punctuations \"\"\"\n",
        "  # lowercase\n",
        "  sent = sent.lower()\n",
        "  # remove punctuation and stopwords\n",
        "  sent = ''.join([c for c in sent if c not in punc]) \n",
        "  tokens = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  # pad\n",
        "  tokens.insert(0, '<s>')\n",
        "  tokens.append('</s>')\n",
        "  # pad the token with special symbols\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == token:\n",
        "      tokens.insert(i, '_START')\n",
        "      tokens.insert(i+2, '_END')\n",
        "      break\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def get_complexity_level(n):\n",
        "  \"\"\" map complexity to corresponding level \"\"\"\n",
        "  # 1: 0, 2: 0:25, 3: 0:5, 4: 0:75, 5: 1\n",
        "  # To use cross entropy, each label has to minus 1\n",
        "  if n <= 0:\n",
        "    return 0\n",
        "  elif n <= 0.25:\n",
        "    return 1\n",
        "  elif n <= 0.5:\n",
        "    return 2\n",
        "  elif n <= 0.75:\n",
        "    return 3 \n",
        "  return 4\n",
        "\n",
        "def preprocess(df):\n",
        "  data = df[['sentence', 'token', 'complexity']]\n",
        "  data['tokenized_sentence'] = data.apply(lambda row: tokenize(row['sentence'], row['token'], punc, stop_words), axis=1)\n",
        "  data['complexity'] = data['complexity'].map(lambda x: get_complexity_level(x))\n",
        "  data = data.drop(columns=['sentence'])\n",
        "  return data\n",
        "\n",
        "train_data = preprocess(df)\n",
        "test_data = preprocess(test)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0</td>\n",
              "      <td>[&lt;s&gt;, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0</td>\n",
              "      <td>[&lt;s&gt;, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[&lt;s&gt;, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[&lt;s&gt;, put, _START, brothers, _END, far, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                   tokenized_sentence\n",
              "0     river  ...                                                         [<s>, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, </s>]\n",
              "1  brothers  ...                                                                [<s>, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, </s>]\n",
              "2  brothers  ...                  [<s>, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, </s>]\n",
              "3  brothers  ...  [<s>, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, </s>]\n",
              "4  brothers  ...                                                                                                        [<s>, put, _START, brothers, _END, far, </s>]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7zco41RQrgD"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jpf2sEdQraL",
        "outputId": "0b200e0c-2784-4537-fa7f-1c146679c121"
      },
      "source": [
        "# take a look at the number of instances for different complexities\n",
        "temp1 = train_data[train_data['complexity'] == 0]\n",
        "temp2 = train_data[train_data['complexity'] == 1]\n",
        "temp3 = train_data[train_data['complexity'] == 2]\n",
        "temp4 = train_data[train_data['complexity'] == 3]\n",
        "temp5 = train_data[train_data['complexity'] == 4]\n",
        "\n",
        "print(\"1: \")\n",
        "print(temp1.shape)\n",
        "print(\"2: \")\n",
        "print(temp2.shape)\n",
        "print(\"3: \")\n",
        "print(temp3.shape)\n",
        "print(\"4: \")\n",
        "print(temp4.shape)\n",
        "print(\"5: \")\n",
        "print(temp5.shape)\n",
        "\n",
        "# there are very few 1 and 5 in the training set"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: \n",
            "(18, 3)\n",
            "2: \n",
            "(3251, 3)\n",
            "3: \n",
            "(3755, 3)\n",
            "4: \n",
            "(617, 3)\n",
            "5: \n",
            "(21, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSGGceDJQo-0"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "l4fnXnBekvW3",
        "outputId": "1554aba4-2cbf-4b44-ae86-603fc0c39e45"
      },
      "source": [
        "# convert words to index for training and testing purpose\n",
        "sentences = train_data['tokenized_sentence'].to_list()\n",
        "temp = []\n",
        "for sent in sentences:\n",
        "  temp += sent\n",
        "temp = set(temp)\n",
        "# for words that are unknown\n",
        "temp.add('_UNKNOWN') \n",
        "temp.add('_PADDING')\n",
        "print(len(temp))\n",
        "\n",
        "# need to pad sentences to the same length\n",
        "lengths = [len(sent) for sent in sentences]\n",
        "pad_length = max(lengths)\n",
        "print(pad_length)\n",
        "\n",
        "# construct dictionaries\n",
        "word2index = {}\n",
        "index2word = {}\n",
        "for i, word in enumerate(temp):\n",
        "  word2index[word] = i\n",
        "  index2word[i] = word\n",
        "\n",
        "def word_to_index(sentence):\n",
        "  # sentence: a list of strings\n",
        "  r = []\n",
        "  for word in sentence:\n",
        "    if word in word2index:\n",
        "      r.append(word2index[word])\n",
        "    else:\n",
        "      r.append(word2index['_UNKNOWN'])\n",
        "  diff = pad_length - len(sentence)\n",
        "  pad_index = word2index['_PADDING']\n",
        "  for i in range(diff):\n",
        "    r.append(pad_index)\n",
        "  return r\n",
        "\n",
        "train_data['number_sentence'] = train_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "test_data['number_sentence'] = test_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "\n",
        "train_data = train_data.drop(columns=['tokenized_sentence'])\n",
        "test_data = test_data.drop(columns=['tokenized_sentence'])\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14826\n",
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>number_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0</td>\n",
              "      <td>[555, 9482, 8103, 5126, 11847, 76, 3320, 5867, 8066, 12022, 12345, 9876, 5661, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0</td>\n",
              "      <td>[555, 8338, 11395, 5126, 13530, 76, 8905, 3907, 8822, 621, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[555, 525, 10151, 2171, 14695, 3801, 3868, 13027, 7611, 5706, 13279, 5126, 13530, 76, 1048, 6870, 1798, 1221, 9336, 10653, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>1</td>\n",
              "      <td>[555, 4489, 9119, 11055, 11060, 3570, 5126, 13530, 76, 611, 7026, 5778, 6295, 11489, 11053, 11549, 5778, 10322, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>2</td>\n",
              "      <td>[555, 5818, 5126, 13530, 76, 12832, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                   number_sentence\n",
              "0     river  ...                  [555, 9482, 8103, 5126, 11847, 76, 3320, 5867, 8066, 12022, 12345, 9876, 5661, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "1  brothers  ...                          [555, 8338, 11395, 5126, 13530, 76, 8905, 3907, 8822, 621, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "2  brothers  ...  [555, 525, 10151, 2171, 14695, 3801, 3868, 13027, 7611, 5706, 13279, 5126, 13530, 76, 1048, 6870, 1798, 1221, 9336, 10653, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "3  brothers  ...     [555, 4489, 9119, 11055, 11060, 3570, 5126, 13530, 76, 611, 7026, 5778, 6295, 11489, 11053, 11549, 5778, 10322, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "4  brothers  ...                                 [555, 5818, 5126, 13530, 76, 12832, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNw9bjg2Luda",
        "outputId": "11d4ae41-26b8-44b2-d429-885e6b8a0b75"
      },
      "source": [
        "# do a simple check\n",
        "print(df.shape)\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(len(word2index.keys()))\n",
        "print(len(index2word.keys()))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7662, 5)\n",
            "(7662, 3)\n",
            "(917, 3)\n",
            "14826\n",
            "14826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL3ASkWeeGZo",
        "outputId": "0e054eca-ea6b-4a35-b658-2416ee6ae6b2"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTh_SM0eGXC"
      },
      "source": [
        "# prepare the batch loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOaqYVuaDuQ_"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word, adding word length and frequency \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 2))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      # initialize a random vector\n",
        "      weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word]])), axis=0 ) # concatenate 2 1d arrays\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmDPzyUOeGUo"
      },
      "source": [
        "# the LSTM class\n",
        "class ComplexityNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)       \n",
        "        out = self.fc(lstm_out[:, -1, :])      \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFAZtgekeGSh",
        "outputId": "f6011325-7b8e-4422-ed18-0e1b5ee77047"
      },
      "source": [
        "# some parameters\n",
        "vocab_size = len(word2index) + 1\n",
        "output_size = 5\n",
        "# embedding dim is not needed because it can be obtained from weights_matrix\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNet(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNet(\n",
            "  (embedding): Embedding(14826, 102)\n",
            "  (lstm): LSTM(102, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eggcDcSeGQJ",
        "outputId": "ce54b22a-8825-4dab-977c-1ce7569b609e"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.933403...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.962250...\n",
            "Epoch: 14/100... Step: 768... Loss: 1.064762...\n",
            "Epoch: 18/100... Step: 1024... Loss: 1.067307...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.921529...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.967771...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.865579...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.870428...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.953858...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.940972...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.985936...\n",
            "Epoch: 53/100... Step: 3072... Loss: 1.051221...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.929494...\n",
            "Epoch: 61/100... Step: 3584... Loss: 1.073398...\n",
            "Epoch: 66/100... Step: 3840... Loss: 1.073839...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.922543...\n",
            "Epoch: 74/100... Step: 4352... Loss: 1.053222...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.912626...\n",
            "Epoch: 83/100... Step: 4864... Loss: 1.062798...\n",
            "Epoch: 87/100... Step: 5120... Loss: 1.046551...\n",
            "Epoch: 92/100... Step: 5376... Loss: 1.061265...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.989019...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.946830...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1997iM_psXaP"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_classification_no_Glove.pt\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASr80p94sXPA",
        "outputId": "327f40b9-b334-4ef3-ec76-91040ce5b5f2"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average; different from the cross entropy used in training\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - ( fetched_output[i].index( max(fetched_output[i])) ) )\n",
        "      if diff == 0:\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 2, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
            "Test loss: 0.530\n",
            "Test accuracy: 46.456%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w52OdjZtBS_"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29TcMd795Kyg"
      },
      "source": [
        "#LSTM classification with 5 classes and pre-trained Glove embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXb6AzSzSYJT"
      },
      "source": [
        "# restart the loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zi2FuT5tBQn"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in glove, adding word length and frequency \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 2))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = torch.cat( (glove[word], torch.Tensor([len(word), count_word_frequency[word]])) ) \n",
        "      except KeyError:\n",
        "          # initialize a random vector\n",
        "          weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word]])), axis=0 ) # concatenate 2 1d arrays\n",
        "      \n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3EQOW-g5LON"
      },
      "source": [
        "# The model; inherits from the previous model\n",
        "class ComplexityNetGlove(ComplexityNet):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNetGlove, self).__init__(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True) # use the Glove\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyFDmkvv5pQ1",
        "outputId": "9336c0bf-c02a-4069-a198-0f86e76825f1"
      },
      "source": [
        "vocab_size = len(word2index) + 1\n",
        "output_size = 5\n",
        "\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNetGlove(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNetGlove(\n",
            "  (embedding): Embedding(14826, 102)\n",
            "  (lstm): LSTM(102, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5F9qH-4v5pGn",
        "outputId": "814e61f4-08dc-44fb-e7c1-21fd74f20dbb"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.986544...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.881576...\n",
            "Epoch: 14/100... Step: 768... Loss: 1.056218...\n",
            "Epoch: 18/100... Step: 1024... Loss: 1.051983...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.906515...\n",
            "Epoch: 27/100... Step: 1536... Loss: 1.139358...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.834727...\n",
            "Epoch: 35/100... Step: 2048... Loss: 1.085973...\n",
            "Epoch: 40/100... Step: 2304... Loss: 1.073400...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.966388...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.909546...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.912064...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.977586...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.920862...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.884171...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.851468...\n",
            "Epoch: 74/100... Step: 4352... Loss: 1.063052...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.853992...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.997677...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.876488...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.951067...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.987191...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.924768...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOadn6Uv5o_G"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_classification_with_Glove.pt\")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPj_sgcM7oJ2",
        "outputId": "00617f32-cada-4650-f265-933861b7a18a"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - ( fetched_output[i].index( max(fetched_output[i])) ) )\n",
        "      if diff == 0:\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 2, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 2, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0]\n",
            "Test loss: 0.531\n",
            "Test accuracy: 46.347%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Kojg1RJswgx"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpmgJ_Y7tEWe"
      },
      "source": [
        "# LSTM with continuous complexities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWKa5VjeyJsA"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "DGZCGk8eyKLE",
        "outputId": "8864cb0f-de40-4e93-e244-f85929458099"
      },
      "source": [
        "# tokenize sentences\n",
        "\n",
        "def tokenize(sent, token, punc, stop_words):\n",
        "  \"\"\" lowercase, padded, remove stopwords and punctuations \"\"\"\n",
        "  # lowercase\n",
        "  sent = sent.lower()\n",
        "  # remove punctuation and stopwords\n",
        "  sent = ''.join([c for c in sent if c not in punc]) \n",
        "  tokens = [word for word in sent.split(' ') if (word.isalpha() and word not in stop_words)]\n",
        "  # pad\n",
        "  tokens.insert(0, '<s>')\n",
        "  tokens.append('</s>')\n",
        "  # pad the token with special symbols\n",
        "  for i in range(len(tokens)):\n",
        "    if tokens[i] == token:\n",
        "      tokens.insert(i, '_START')\n",
        "      tokens.insert(i+2, '_END')\n",
        "      break\n",
        "\n",
        "  return tokens\n",
        "\n",
        "def preprocess(df):\n",
        "  data = df[['sentence', 'token', 'complexity']]\n",
        "  data['tokenized_sentence'] = data.apply(lambda row: tokenize(row['sentence'], row['token'], punc, stop_words), axis=1)\n",
        "  data = data.drop(columns=['sentence'])\n",
        "  return data\n",
        "\n",
        "train_data = preprocess(df)\n",
        "test_data = preprocess(test)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>tokenized_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[&lt;s&gt;, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[&lt;s&gt;, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>[&lt;s&gt;, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>[&lt;s&gt;, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>[&lt;s&gt;, put, _START, brothers, _END, far, &lt;/s&gt;]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                   tokenized_sentence\n",
              "0     river  ...                                                         [<s>, behold, came, _START, river, _END, seven, cattle, sleek, fat, fed, marsh, grass, </s>]\n",
              "1  brothers  ...                                                                [<s>, fellow, bondservant, _START, brothers, _END, prophets, keep, words, book, </s>]\n",
              "2  brothers  ...                  [<s>, man, lord, land, said, us, know, honest, men, leave, one, _START, brothers, _END, take, grain, famine, houses, go, way, </s>]\n",
              "3  brothers  ...  [<s>, shimei, sixteen, sons, six, daughters, _START, brothers, _END, didnt, many, children, neither, family, multiply, like, children, judah, </s>]\n",
              "4  brothers  ...                                                                                                        [<s>, put, _START, brothers, _END, far, </s>]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "c4XYi3COyKLF",
        "outputId": "774469e3-4f24-4029-caba-85cba0c1520c"
      },
      "source": [
        "# convert words to index for training and testing purpose\n",
        "sentences = train_data['tokenized_sentence'].to_list()\n",
        "temp = []\n",
        "for sent in sentences:\n",
        "  temp += sent\n",
        "temp = set(temp)\n",
        "# for words that are unknown\n",
        "temp.add('_UNKNOWN') \n",
        "temp.add('_PADDING')\n",
        "print(len(temp))\n",
        "\n",
        "# need to pad sentences to the same length\n",
        "lengths = [len(sent) for sent in sentences]\n",
        "pad_length = max(lengths)\n",
        "print(pad_length)\n",
        "\n",
        "# construct dictionaries\n",
        "word2index = {}\n",
        "index2word = {}\n",
        "for i, word in enumerate(temp):\n",
        "  word2index[word] = i\n",
        "  index2word[i] = word\n",
        "\n",
        "def word_to_index(sentence):\n",
        "  # sentence: a list of strings\n",
        "  r = []\n",
        "  for word in sentence:\n",
        "    if word in word2index:\n",
        "      r.append(word2index[word])\n",
        "    else:\n",
        "      r.append(word2index['_UNKNOWN'])\n",
        "  diff = pad_length - len(sentence)\n",
        "  pad_index = word2index['_PADDING']\n",
        "  for i in range(diff):\n",
        "    r.append(pad_index)\n",
        "  return r\n",
        "\n",
        "train_data['number_sentence'] = train_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "test_data['number_sentence'] = test_data['tokenized_sentence'].map(lambda sent: word_to_index(sent))\n",
        "\n",
        "train_data = train_data.drop(columns=['tokenized_sentence'])\n",
        "test_data = test_data.drop(columns=['tokenized_sentence'])\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14826\n",
            "118\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>complexity</th>\n",
              "      <th>number_sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>river</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[555, 9482, 8103, 5126, 11847, 76, 3320, 5867, 8066, 12022, 12345, 9876, 5661, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[555, 8338, 11395, 5126, 13530, 76, 8905, 3907, 8822, 621, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>[555, 525, 10151, 2171, 14695, 3801, 3868, 13027, 7611, 5706, 13279, 5126, 13530, 76, 1048, 6870, 1798, 1221, 9336, 10653, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.150000</td>\n",
              "      <td>[555, 4489, 9119, 11055, 11060, 3570, 5126, 13530, 76, 611, 7026, 5778, 6295, 11489, 11053, 11549, 5778, 10322, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>brothers</td>\n",
              "      <td>0.263889</td>\n",
              "      <td>[555, 5818, 5126, 13530, 76, 12832, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      token  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                   number_sentence\n",
              "0     river  ...                  [555, 9482, 8103, 5126, 11847, 76, 3320, 5867, 8066, 12022, 12345, 9876, 5661, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "1  brothers  ...                          [555, 8338, 11395, 5126, 13530, 76, 8905, 3907, 8822, 621, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "2  brothers  ...  [555, 525, 10151, 2171, 14695, 3801, 3868, 13027, 7611, 5706, 13279, 5126, 13530, 76, 1048, 6870, 1798, 1221, 9336, 10653, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "3  brothers  ...     [555, 4489, 9119, 11055, 11060, 3570, 5126, 13530, 76, 611, 7026, 5778, 6295, 11489, 11053, 11549, 5778, 10322, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "4  brothers  ...                                 [555, 5818, 5126, 13530, 76, 12832, 248, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, ...]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs1y-euItBtz"
      },
      "source": [
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-SaakyDHMyu"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word, adding word length and frequency \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 2))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      # initialize a random vector\n",
        "      weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word]])), axis=0 ) # concatenate 2 1d arrays\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXeyJqECtBt0"
      },
      "source": [
        "# the LSTM class\n",
        "class ComplexityNet(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNet, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)       \n",
        "        out = self.fc(lstm_out[:, -1, :])      \n",
        "        return out, hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
        "        return hidden"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3nsx9L9tBt1",
        "outputId": "630c95a1-6b2d-4afb-8d5a-0e2b9da49cfc"
      },
      "source": [
        "# some parameters\n",
        "vocab_size = len(word2index) + 1\n",
        "output_size = 1\n",
        "\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNet(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNet(\n",
            "  (embedding): Embedding(14826, 102)\n",
            "  (lstm): LSTM(102, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8vIp2rStBt1",
        "outputId": "6c07b4c4-3fd6-400e-9a13-452c32817d71"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.093377...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.107981...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.119913...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.118705...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.099511...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.102816...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.108866...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.109073...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.095132...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.102492...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.104739...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.100262...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.103227...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.120291...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.115958...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.090610...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.101611...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.102132...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.107195...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.103970...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.094018...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.102811...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.101959...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcTUYus9tBt2"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_continuous_no_Glove.pt\")"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfSpouH1tBt2",
        "outputId": "0637262d-e7b2-4697-daf9-461dbea5b3ec"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average; different from the cross entropy used in training\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - fetched_output[i] )\n",
        "      if diff <= 0.05: # a difference that is small\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.015027077574478964, 0.09222006003061933, 0.003331171141730427, 0.12444660663604734, 0.05222438441382515, 0.024446606636047363, 0.004965158069835052, 0.24666882885826957, 0.05222438441382515, 0.15412482193538124, 0.13731809924630556, 0.04222006003061929, 0.011946606636047352, 0.14742839336395264, 0.19777561558617485, 0.001869182837636818, 0.044997837808397056, 0.20055339336395261, 0.01308297027241101, 0.24025927571689376, 0.041342867048163146, 0.08269625050680973, 0.19944660663604735, 0.06016089235033306, 0.05222438441382515, 0.011267679078238335, 0.10257160663604736, 0.024446606636047363, 0.07444660663604735, 0.35055339336395264, 0.06483910764966694, 0.02100793881849805, 0.11373232092176166, 0.031108948919508217, 0.05385837134192972, 0.07444660663604735, 0.2077799399693807, 0.13731809924630556, 0.45769625050680973, 0.031108948919508217, 0.1922200600306193, 0.027636726697286007, 0.27444660663604736, 0.06016089235033306, 0.10777993996938068, 0.07707818558341575, 0.11617839336395264, 0.04944660663604736, 0.16672986395218792, 0.07444660663604735, 0.19777561558617485, 0.22555339336395264, 0.03833549552493626, 0.1779343457449049, 0.05222438441382515, 0.002707476201264769, 0.024446606636047363, 0.006803393363952637, 0.031108948919508217, 0.08269625050680973, 0.10339397505710002, 0.4505533933639526, 0.034376922775717356, 0.12555339336395266, 0.015027077574478964, 0.03805339336395264, 0.05367839336395264, 0.031108948919508217, 0.03833549552493626, 0.036351368540809254, 0.03833549552493626, 0.1422200600306192, 0.18983910764966694, 0.1673037494931902, 0.09023608032025787, 0.0065894637789044586, 0.08666450447506385, 0.024446606636047363, 0.04007160663604736, 0.052476470287029564, 0.022428393363952637, 0.06483910764966694, 0.19923760389026846, 0.07707818558341575, 0.03915248898898857, 0.06194660663604737, 0.05569660663604736, 0.44777561558617485, 0.02555339336395268, 0.08327013604781208, 0.26126767907823834, 0.11268190075369444, 0.008886726697285963, 0.11373232092176166, 0.25680339336395264, 0.10055339336395264, 0.04222006003061929, 0.12555339336395266, 0.010557717747158524, 0.06392029084657369, 0.2922200600306193, 0.07277561558617485, 0.12444660663604734, 0.04230374949319021, 0.11819660663604736, 0.14944660663604736, 0.006803393363952637, 0.06378868748159972, 0.07555339336395261, 0.3630533933639526, 0.07555339336395261, 0.024446606636047363, 0.06378868748159972, 0.07079148860204787, 0.013053393363952615, 0.06378868748159972, 0.022428393363952637, 0.47555339336395264, 0.04222006003061929, 0.08269625050680973, 0.009740724283106239, 0.0065894637789044586, 0.009740724283106239, 0.07555339336395261, 0.18143574630512915, 0.0065894637789044586, 0.06016089235033306, 0.03833549552493626, 0.015027077574478964, 0.2533311711417304, 0.08000216219160297, 0.09797601840075326, 0.011288711899205273, 0.1420936654595768, 0.05888672669728601, 0.011267679078238335, 0.2108475110110115, 0.14742839336395264, 0.47555339336395264, 0.1922200600306193, 0.024446606636047363, 0.02100793881849805, 0.024446606636047363, 0.17027993996938068, 0.024446606636047363, 0.06930339336395264, 0.03833549552493626, 0.041342867048163146, 0.02912482193538124, 0.21166450447506385, 0.06856425369487087, 0.06378868748159972, 0.06930339336395264, 0.08126478845422921, 0.18516089235033306, 0.007779939969380645, 0.06016089235033306, 0.2696710404227761, 0.036946606636047374, 0.13555771774715847, 0.032371575182134404, 0.024446606636047363, 0.09587517806461882, 0.08694660663604736, 0.06305339336395266, 0.001869182837636818, 0.07079148860204787, 0.09587517806461882, 0.11819660663604736, 0.17867839336395264, 0.004965158069835052, 0.11819660663604736, 0.04007160663604736, 0.05385837134192972, 0.07801803520747591, 0.05385837134192972, 0.05888672669728601, 0.26126767907823834, 0.07801803520747591, 0.06856425369487087, 0.024446606636047363, 0.1079063345404232, 0.24025927571689376, 0.05222438441382515, 0.11617839336395264, 0.06016089235033306, 0.041113273302714054, 0.036946606636047374, 0.08918975700031634, 0.0507623961097316, 0.07849456983454084, 0.19944660663604735, 0.044997837808397056, 0.09111327330271404, 0.041342867048163146, 0.08327013604781208, 0.10713234073237365, 0.23871128810079478, 0.07444660663604735, 0.14944660663604736, 0.05777993996938069, 0.04222006003061929, 0.036946606636047374, 0.041113273302714054, 0.03833549552493626, 0.1079063345404232, 0.010557717747158524, 0.22555339336395264, 0.27242839336395264, 0.10888672669728594, 0.09111327330271404, 0.13158946377890451, 0.008821606636047363, 0.03805339336395264, 0.2799012194509092, 0.0065894637789044586, 0.008821606636047363, 0.09111327330271404, 0.05222438441382515, 0.07444660663604735, 0.07801803520747591, 0.24117839336395264, 0.25888672669728596, 0.10055339336395264, 0.31483910764966694, 0.024446606636047363, 0.04230374949319021, 0.17867839336395264, 0.003331171141730427, 0.11305339336395265, 0.24222006003061936, 0.013053393363952615, 0.16305339336395264, 0.037604501372889454, 0.024446606636047363, 0.0005533933639526589, 0.02818497231132111, 0.08000216219160297, 0.02912482193538124, 0.11373232092176166, 0.024446606636047363, 0.19944660663604735, 0.024446606636047363, 0.024446606636047363, 0.06856425369487087, 0.05222438441382515, 0.13555771774715847, 0.09797601840075326, 0.05450076178500529, 0.0125418447312855, 0.037604501372889454, 0.003331171141730427, 0.308886726697286, 0.0507623961097316, 0.25055339336395266, 0.06290814509758577, 0.008821606636047363, 0.10055339336395264, 0.159061991251432, 0.04908280512865848, 0.22555339336395264, 0.024446606636047363, 0.1039920611815019, 0.1422200600306192, 0.15412482193538124, 0.022428393363952637, 0.06194660663604737, 0.041113273302714054, 0.06378868748159972, 0.05367839336395264, 0.04908280512865848, 0.17555339336395265, 0.1079063345404232, 0.08327013604781208, 0.11617839336395264, 0.141113273302714, 0.22444660663604737, 0.013053393363952615, 0.07444660663604735, 0.08694660663604736, 0.19430339336395264, 0.09587517806461882, 0.4020239815992468, 0.024446606636047363, 0.03507720288776217, 0.47555339336395264, 0.036351368540809254, 0.1520239815992468, 0.08269625050680973, 0.07444660663604735, 0.024446606636047363, 0.1922200600306193, 0.024446606636047363, 0.05055339336395265, 0.015027077574478964, 0.09587517806461882, 0.34093800874856806, 0.05385837134192972, 0.06856425369487087, 0.08000216219160297, 0.1520239815992468, 0.09944660663604737, 0.09320045218748207, 0.09222006003061933, 0.12833117114173043, 0.017220060030619266, 0.19430339336395264, 0.16333549552493626, 0.14944660663604736, 0.037604501372889454, 0.06016089235033306, 0.03805339336395264, 0.024446606636047363, 0.08694660663604736, 0.037604501372889454, 0.3005533933639526, 0.5033311711417303, 0.04230374949319021, 0.09587517806461882, 0.024446606636047363, 0.034376922775717356, 0.05777993996938069, 0.017220060030619266, 0.15888672669728598, 0.05888672669728601, 0.17444660663604739, 0.03915248898898857, 0.06646248427304352, 0.19777561558617485, 0.05569660663604736, 0.0006370828265235273, 0.0125418447312855, 0.05777993996938069, 0.1520239815992468, 0.09111327330271404, 0.12555339336395266, 0.05055339336395265, 0.07555339336395261, 0.35055339336395264, 0.25680339336395264, 0.08666450447506385, 0.13180339336395264, 0.02555339336395268, 0.05777993996938069, 0.041113273302714054, 0.14944660663604736, 0.17444660663604739, 0.12444660663604734, 0.19944660663604735, 0.06856425369487087, 0.22555339336395264, 0.08666450447506385, 0.04908280512865848, 0.037604501372889454, 0.0065894637789044586, 0.07277561558617485, 0.04007160663604736, 0.141113273302714, 0.04222006003061929, 0.27242839336395264, 0.003331171141730427, 0.10777993996938068, 0.09023608032025787, 0.10055339336395264, 0.034376922775717356, 0.07707818558341575, 0.10055339336395264, 0.29698196479252403, 0.08918975700031634, 0.03507720288776217, 0.08666450447506385, 0.031108948919508217, 0.007779939969380645, 0.05777993996938069, 0.08492839336395264, 0.0005533933639526589, 0.006803393363952637, 0.25888672669728596, 0.046981964792524034, 0.03833549552493626, 0.25680339336395264, 0.09111327330271404, 0.05555339336395265, 0.02555339336395268, 0.07707818558341575, 0.08694660663604736, 0.08694660663604736, 0.024446606636047363, 0.09111327330271404, 0.02555339336395268, 0.17555339336395265, 0.28437692277571736, 0.07277561558617485, 0.14944660663604736, 0.07444660663604735, 0.32079148860204787, 0.09944660663604737, 0.11097006003061921, 0.05055339336395265, 0.044997837808397056, 0.06378868748159972, 0.04944660663604736, 0.041113273302714054, 0.024446606636047363, 0.05222438441382515, 0.11373232092176166, 0.12738778310663562, 0.04230374949319021, 0.05888672669728601, 0.09797601840075326, 0.011288711899205273, 0.04908280512865848, 0.024446606636047363, 0.009740724283106239, 0.13180339336395264, 0.09222006003061933, 0.05367839336395264, 0.024446606636047363, 0.06378868748159972, 0.11841053622109554, 0.27100793881849805, 0.17444660663604739, 0.003331171141730427, 0.07707818558341575, 0.22444660663604737, 0.09944660663604737, 0.06856425369487087, 0.06856425369487087, 0.18143574630512915, 0.10339397505710002, 0.034376922775717356, 0.13805339336395261, 0.11373232092176166, 0.034376922775717356, 0.18983910764966694, 0.09389105108049176, 0.008886726697285963, 0.14944660663604736, 0.15412482193538124, 0.07132160663604736, 0.11819660663604736, 0.29499783780839706, 0.024446606636047363, 0.04230374949319021, 0.008886726697285963, 0.05777993996938069, 0.05888672669728601, 0.04908280512865848, 0.24341053622109554, 0.22087517806461882, 0.024446606636047363, 0.031108948919508217, 0.10777993996938068, 0.12261221689336443, 0.0065894637789044586, 0.046981964792524034, 0.08694660663604736, 0.08694660663604736, 0.07132160663604736, 0.006803393363952637, 0.15679954781251793, 0.25055339336395266, 0.09111327330271404, 0.02555339336395268, 0.17555339336395265, 0.06930339336395264, 0.13344813020605795, 0.0005533933639526589, 0.05569660663604736, 0.04007160663604736, 0.05569660663604736, 0.04007160663604736, 0.27912482193538124, 0.08918975700031634, 0.09389105108049176, 0.08666450447506385, 0.10257160663604736, 0.008886726697285963, 0.05450076178500529, 0.22444660663604737, 0.05385837134192972, 0.04007160663604736, 0.14742839336395264, 0.10777993996938068, 0.09397444599553162, 0.05777993996938069, 0.08492839336395264, 0.024446606636047363, 0.024446606636047363, 0.05222438441382515, 0.03915248898898857, 0.09111327330271404, 0.12738778310663562, 0.06378868748159972, 0.05888672669728601, 0.022428393363952637, 0.01967104042277612, 0.03833549552493626, 0.06250991510308312, 0.42867839336395264, 0.04007160663604736, 0.05385837134192972, 0.08666450447506385, 0.22444660663604737, 0.03833549552493626, 0.12166882885826957, 0.04286108567164493, 0.022428393363952637, 0.17198196479252403, 0.28805339336395264, 0.12261221689336443, 0.008821606636047363, 0.07132160663604736, 0.1422200600306192, 0.03805339336395264, 0.07801803520747591, 0.036946606636047374, 0.19294469771177875, 0.031108948919508217, 0.07849456983454084, 0.05569660663604736, 0.043677375866816553, 0.03833549552493626, 0.07801803520747591, 0.07707818558341575, 0.009740724283106239, 0.21468382814656134, 0.0017193339087746562, 0.031108948919508217, 0.08805339336395263, 0.05569660663604736, 0.3630533933639526, 0.024446606636047363, 0.022428393363952637, 0.07555339336395261, 0.33492839336395264, 0.09587517806461882, 0.13158946377890451, 0.36841053622109554, 0.037604501372889454, 0.03805339336395264, 0.15679954781251793, 0.12738778310663562, 0.22555339336395264, 0.006803393363952637, 0.007779939969380645, 0.022428393363952637, 0.07206565425509495, 0.0005533933639526589, 0.023172440983000364, 0.03805339336395264, 0.13180339336395264, 0.05888672669728601, 0.05777993996938069, 0.024446606636047363, 0.07444660663604735, 0.11655186979394211, 0.024446606636047363, 0.15610894891950822, 0.024446606636047363, 0.017220060030619266, 0.07337948032047431, 0.11194660663604739, 0.4005533933639527, 0.08327013604781208, 0.27444660663604736, 0.07801803520747591, 0.05222438441382515, 0.03833549552493626, 0.18143574630512915, 0.046981964792524034, 0.13382160663604736, 0.044997837808397056, 0.024446606636047363, 0.1715054301654591, 0.39222006003061927, 0.008821606636047363, 0.09111327330271404, 0.024446606636047363, 0.06930339336395264, 0.22444660663604737, 0.05385837134192972, 0.046981964792524034, 0.22555339336395264, 0.10257160663604736, 0.13180339336395264, 0.09023608032025787, 0.07782612063667993, 0.06856425369487087, 0.2640149318254911, 0.10055339336395264, 0.12166882885826957, 0.024446606636047363, 0.024446606636047363, 0.18516089235033306, 0.24117839336395264, 0.12444660663604734, 0.05367839336395264, 0.024446606636047363, 0.004965158069835052, 0.024446606636047363, 0.14944660663604736, 0.3005533933639526, 0.04230374949319021, 0.07849456983454084, 0.05777993996938069, 0.024446606636047363, 0.11819660663604736, 0.20888672669728592, 0.02912482193538124, 0.03805339336395264, 0.10055339336395264, 0.008886726697285963, 0.14742839336395264, 0.011267679078238335, 0.09389105108049176, 0.13344813020605795, 0.07132160663604736, 0.10257160663604736, 0.25680339336395264, 0.009740724283106239, 0.05888672669728601, 0.05222438441382515, 0.15055339336395263, 0.09320045218748207, 0.35055339336395264, 0.3366645044750638, 0.13180339336395264, 0.16305339336395264, 0.05385837134192972, 0.14944660663604736, 0.17444660663604739, 0.13382160663604736, 0.07206565425509495, 0.044997837808397056, 0.006803393363952637, 0.06016089235033306, 0.20992839336395264, 0.06930339336395264, 0.05055339336395265, 0.06305339336395266, 0.07555339336395261, 0.04373521154577087, 0.09797601840075326, 0.07444660663604735, 0.08666450447506385, 0.04007160663604736, 0.04222006003061929, 0.2077799399693807, 0.07555339336395261, 0.05509884790940722, 0.007779939969380645, 0.01967104042277612, 0.19294469771177875, 0.23944228225284148, 0.06378868748159972, 0.03805339336395264, 0.024446606636047363, 0.004965158069835052, 0.09320045218748207, 0.023172440983000364, 0.09222006003061933, 0.02555339336395268, 0.05385837134192972, 0.003613273302713993, 0.024446606636047363, 0.11268190075369444, 0.024446606636047363, 0.02555339336395268, 0.10257160663604736, 0.06555339336395266, 0.024446606636047363, 0.041113273302714054, 0.14944660663604736, 0.05777993996938069, 0.27444660663604736, 0.044997837808397056, 0.10055339336395264, 0.08694660663604736, 0.09320045218748207, 0.05888672669728601, 0.033245701056260346, 0.004965158069835052, 0.03805339336395264, 0.19944660663604735, 0.19111327330271405, 0.06378868748159972, 0.07801803520747591, 0.22555339336395264, 0.1346443024548617, 0.07801803520747591, 0.11819660663604736, 0.044997837808397056, 0.16999783780839706, 0.09797601840075326, 0.05569660663604736, 0.017220060030619266, 0.036946606636047374, 0.02555339336395268, 0.024446606636047363, 0.25888672669728596, 0.08694660663604736, 0.3137886874815997, 0.02555339336395268, 0.21166450447506385, 0.07555339336395261, 0.2990828051286585, 0.024446606636047363, 0.07444660663604735, 0.11373232092176166, 0.03805339336395264, 0.29698196479252403, 0.003331171141730427, 0.09389105108049176, 0.12166882885826957, 0.017220060030619266, 0.13731809924630556, 0.13731809924630556, 0.03915248898898857, 0.08213891432835507, 0.04077078466830042, 0.04230374949319021, 0.09222006003061933, 0.06765865652184733, 0.1922200600306193, 0.008886726697285963, 0.06016089235033306, 0.17444660663604739, 0.08694660663604736, 0.034376922775717356, 0.09944660663604737, 0.2533311711417304, 0.0005533933639526589, 0.08694660663604736, 0.24666882885826957, 0.022428393363952637, 0.04286108567164493, 0.08666450447506385, 0.024446606636047363, 0.09944660663604737, 0.2380533933639526, 0.01967104042277612, 0.0005533933639526589, 0.15777993996938072, 0.05222438441382515, 0.031108948919508217, 0.11444228225284142, 0.05569660663604736, 0.010557717747158524, 0.04222006003061929, 0.07707818558341575, 0.01308297027241101, 0.006803393363952637, 0.03833549552493626, 0.20888672669728592, 0.11819660663604736, 0.14944660663604736, 0.2108475110110115, 0.16305339336395264, 0.04944660663604736, 0.09320045218748207, 0.10888672669728594, 0.15610894891950822, 0.06290814509758577, 0.02100793881849805, 0.09111327330271404, 0.06611327330271408, 0.14944660663604736, 0.11819660663604736, 0.041342867048163146, 0.05055339336395265, 0.14944660663604736, 0.08000216219160297, 0.034376922775717356, 0.12444660663604734, 0.024446606636047363, 0.02818497231132111, 0.024446606636047363, 0.008886726697285963, 0.09944660663604737, 0.3255533933639526, 0.10777993996938068, 0.05569660663604736, 0.07132160663604736, 0.013053393363952615, 0.07277561558617485, 0.08269625050680973, 0.001869182837636818, 0.017220060030619266, 0.05777993996938069, 0.024446606636047363, 0.18983910764966694, 0.12738778310663562, 0.024446606636047363, 0.20992839336395264, 0.04222006003061929, 0.04825613044557123, 0.30367839336395264, 0.07170723951779873, 0.004965158069835052, 0.10055339336395264, 0.07849456983454084, 0.015027077574478964, 0.17444660663604739, 0.07132160663604736, 0.13180339336395264, 0.21239549862711044, 0.007779939969380645, 0.27444660663604736, 0.17555339336395265, 0.05777993996938069, 0.06392029084657369, 0.007779939969380645, 0.16333549552493626, 0.22555339336395264, 0.07079148860204787, 0.05777993996938069, 0.08918975700031634, 0.10777993996938068, 0.07132160663604736, 0.07277561558617485, 0.024446606636047363, 0.11685774119003955, 0.08492839336395264, 0.3176586565218473, 0.11268190075369444, 0.05367839336395264, 0.05888672669728601, 0.05385837134192972, 0.06611327330271408, 0.14944660663604736, 0.05385837134192972, 0.10055339336395264, 0.08694660663604736, 0.2640149318254911, 0.02818497231132111, 0.03581024299968372, 0.06611327330271408, 0.09111327330271404, 0.07170723951779873, 0.08694660663604736, 0.24117839336395264, 0.003331171141730427, 0.0006370828265235273, 0.03833549552493626, 0.12261221689336443, 0.001869182837636818, 0.015027077574478964, 0.05569660663604736, 0.04007160663604736, 0.011288711899205273, 0.13158946377890451, 0.057055302288221255, 0.1573715751821344, 0.1422200600306192, 0.10055339336395264, 0.17292181441658416, 0.22444660663604737, 0.024446606636047363, 0.308886726697286, 0.05385837134192972, 0.2533311711417304, 0.41999783780839706, 0.015027077574478964, 0.03805339336395264, 0.024446606636047363, 0.09587517806461882, 0.1673037494931902, 0.17555339336395265, 0.10055339336395264, 0.07132160663604736, 0.008821606636047363, 0.12166882885826957, 0.17555339336395265, 0.07444660663604735, 0.17198196479252403, 0.024446606636047363, 0.08694660663604736, 0.06378868748159972, 0.009740724283106239, 0.15888672669728598, 0.0125418447312855, 0.07801803520747591, 0.12444660663604734, 0.07555339336395261, 0.008821606636047363, 0.024446606636047363, 0.043677375866816553, 0.2922200600306193, 0.06392029084657369, 0.04007160663604736, 0.04007160663604736, 0.19111327330271405, 0.05569660663604736, 0.1079063345404232, 0.10777993996938068]\n",
            "Test loss: 0.096\n",
            "Test accuracy: 33.915%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyWnPv58tKww"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxR1pjShtLEg"
      },
      "source": [
        "#LSTM with continuous complexities and with pre-trained Glove embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grGcPB_Tt4Bi"
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSacas2at8FS"
      },
      "source": [
        "# restart the loader\n",
        "train_sentences = np.array(train_data['number_sentence'].to_list())\n",
        "train_labels = np.array(train_data['complexity'].to_list())\n",
        "test_sentences = np.array(test_data['number_sentence'].to_list())\n",
        "test_labels = np.array(test_data['complexity'].to_list())\n",
        "\n",
        "training = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\n",
        "testing = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_loader = DataLoader(training, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(testing, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar2O53uot8FS"
      },
      "source": [
        "def create_weights_matrix_tensor(target_vocab, dimension=100):\n",
        "  \"\"\" create a matrix containing vectors for each word in glove, adding word length and frequency \"\"\"\n",
        "  matrix_len = len(target_vocab)\n",
        "  weights_matrix = np.zeros((matrix_len, dimension + 2))\n",
        "\n",
        "  for i, word in enumerate(target_vocab):\n",
        "      try: \n",
        "          weights_matrix[i] = torch.cat( (glove[word], torch.Tensor([len(word), count_word_frequency[word]])) ) \n",
        "      except KeyError:\n",
        "          # initialize a random vector\n",
        "          weights_matrix[i] = np.concatenate( (np.random.normal(scale=0.6, size=(dimension, )), np.array([len(word), count_word_frequency[word]])), axis=0 ) # concatenate 2 1d arrays\n",
        "  return torch.from_numpy(weights_matrix) # must be a tensor!!\n",
        "\n",
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "  \"\"\" an embedding layer \"\"\"\n",
        "  num_embeddings, embedding_dim = weights_matrix.size()\n",
        "  emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
        "  emb_layer.load_state_dict({'weight': weights_matrix})\n",
        "  if non_trainable:\n",
        "    emb_layer.weight.requires_grad = False\n",
        "\n",
        "  return emb_layer, num_embeddings, embedding_dim"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxCq01M0t8FS"
      },
      "source": [
        "# The model; inherits from the previous model\n",
        "class ComplexityNetGlove(ComplexityNet):\n",
        "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, weights_matrix, drop_prob=0.5):\n",
        "        super(ComplexityNetGlove, self).__init__(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True) # use the Glove\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfdiMdSPt8FT",
        "outputId": "2b21290d-dbe4-45a8-b5f3-d9d4f656aecf"
      },
      "source": [
        "vocab_size = len(word2index) + 1\n",
        "output_size = 1\n",
        "\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "weights_matrix = create_weights_matrix_tensor(word2index.keys())\n",
        "\n",
        "model = ComplexityNetGlove(vocab_size, output_size, hidden_dim, n_layers, weights_matrix)\n",
        "model.to(device)\n",
        "print(model)\n",
        "\n",
        "lr=0.01\n",
        "criterion = nn.L1Loss(reduction='mean')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "epochs = 100\n",
        "counter = 0\n",
        "print_every = 256\n",
        "clip = 5"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ComplexityNetGlove(\n",
            "  (embedding): Embedding(14826, 102)\n",
            "  (lstm): LSTM(102, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUujtF0At8FT",
        "outputId": "cfe2cfc1-e14a-4675-8b92-c6cf7bdb555c"
      },
      "source": [
        "# training\n",
        "model.train()\n",
        "for i in range(epochs):\n",
        "  h = model.init_hidden(batch_size)\n",
        "    \n",
        "  for inputs, labels in train_loader:\n",
        "    counter += 1\n",
        "    h = tuple([e.data for e in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    model.zero_grad()\n",
        "    output, h = model(inputs, h)\n",
        "    # cross entropy for multiple classes\n",
        "    # output is of shape 64 * 5 while labels is of shape 64\n",
        "    loss = criterion(output, labels) \n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    if counter%print_every == 0:\n",
        "      print(\"Epoch: {}/{}...\".format(i+1, epochs), \"Step: {}...\".format(counter), \"Loss: {:.6f}...\".format(loss.item()))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.l1_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 5/100... Step: 256... Loss: 0.104370...\n",
            "Epoch: 9/100... Step: 512... Loss: 0.107615...\n",
            "Epoch: 14/100... Step: 768... Loss: 0.087860...\n",
            "Epoch: 18/100... Step: 1024... Loss: 0.099376...\n",
            "Epoch: 22/100... Step: 1280... Loss: 0.099269...\n",
            "Epoch: 27/100... Step: 1536... Loss: 0.088925...\n",
            "Epoch: 31/100... Step: 1792... Loss: 0.109810...\n",
            "Epoch: 35/100... Step: 2048... Loss: 0.115458...\n",
            "Epoch: 40/100... Step: 2304... Loss: 0.111822...\n",
            "Epoch: 44/100... Step: 2560... Loss: 0.103413...\n",
            "Epoch: 48/100... Step: 2816... Loss: 0.106233...\n",
            "Epoch: 53/100... Step: 3072... Loss: 0.104039...\n",
            "Epoch: 57/100... Step: 3328... Loss: 0.102192...\n",
            "Epoch: 61/100... Step: 3584... Loss: 0.090361...\n",
            "Epoch: 66/100... Step: 3840... Loss: 0.096699...\n",
            "Epoch: 70/100... Step: 4096... Loss: 0.104250...\n",
            "Epoch: 74/100... Step: 4352... Loss: 0.089025...\n",
            "Epoch: 79/100... Step: 4608... Loss: 0.098936...\n",
            "Epoch: 83/100... Step: 4864... Loss: 0.087294...\n",
            "Epoch: 87/100... Step: 5120... Loss: 0.094523...\n",
            "Epoch: 92/100... Step: 5376... Loss: 0.088626...\n",
            "Epoch: 96/100... Step: 5632... Loss: 0.109574...\n",
            "Epoch: 100/100... Step: 5888... Loss: 0.114129...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9me9Lr8mt8FT"
      },
      "source": [
        "# save the model\n",
        "torch.save(model.state_dict(), \"/content/gdrive/My Drive/Colab Notebooks/CS505FinalProject/LSTM_continuous_with_Glove.pt\")"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnw42ciMt8FU",
        "outputId": "e4d39625-2449-48fd-a6e4-5cea24d3ae29"
      },
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "h = model.init_hidden(batch_size)\n",
        "\n",
        "model.eval()\n",
        "for inputs, labels in test_loader:\n",
        "    \n",
        "    h = tuple([each.data for each in h])\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output, h = model(inputs, h)\n",
        "    fetched_labels = labels.data.tolist()\n",
        "    fetched_output = output.squeeze().data.tolist()\n",
        "    #print(len(fetched_labels)) # 32\n",
        "    #print(len(fetched_output[0])) # 5\n",
        "\n",
        "    # absolute mean average\n",
        "    denominator = len(output)\n",
        "    for i in range(denominator):\n",
        "      diff = abs( fetched_labels[i] - fetched_output[i] )\n",
        "      if diff <= 0.05: # a difference that is small\n",
        "        num_correct += 1\n",
        "      test_losses.append(diff)\n",
        "\n",
        "print(test_losses)        \n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.022186251063095908, 0.053001748664038506, 0.12160145574145836, 0.009028356326253761, 0.5104903446303473, 0.009028326523931374, 0.017287462949752808, 0.14447724293260011, 0.10771253705024719, 0.21728743314743043, 0.06140511000857632, 0.022186251063095908, 0.12443032009260996, 0.017287462949752808, 0.00166243314743042, 0.37021256685256954, 0.02978743314743043, 0.29937923351923623, 0.3108375370502472, 0.050620796283086134, 0.02437923351923621, 0.04521253705024719, 0.06604590018590295, 0.1545875370502472, 0.007712566852569602, 0.14228743314743042, 0.0908168747144587, 0.2768301841090707, 0.09382364816135841, 0.031176351838641703, 0.050620766480763746, 0.13395412961641945, 0.013962537050247192, 0.07085889152118136, 0.2188236779636808, 0.10771253705024719, 0.06728743314743041, 0.1922874331474304, 0.06416246294975281, 0.07993478907479179, 0.14180347594347864, 0.026830184109070676, 0.24741841940318832, 0.16728743314743044, 0.02218622126077352, 0.07271253705024722, 0.01728743314743042, 0.3219982513359615, 0.19937923351923625, 0.1780017486640385, 0.013962537050247192, 0.06140508020625393, 0.017287462949752808, 0.13271253705024721, 0.004129568212910717, 0.23950968517197502, 0.0339541296164195, 0.2639625370502472, 0.04506521092520821, 0.07094783116789427, 0.11103746294975281, 0.02978746294975282, 0.18008095810287872, 0.07795066209066481, 0.04153606646201191, 0.07085889152118136, 0.0672874629497528, 0.057712537050247203, 0.05895412961641952, 0.1255696799073901, 0.029587537050247192, 0.24833756685256958, 0.2795875668525696, 0.11506547822671775, 0.177157011297014, 0.304141138281141, 0.017287462949752808, 0.057712537050247203, 0.1922874629497528, 0.09228746294975282, 0.4271569814946916, 0.03291246294975281, 0.19937920371691387, 0.03514457600457327, 0.13549031482802498, 0.40771253705024724, 0.1172874331474304, 0.1827125668525696, 0.0908168747144587, 0.004451667485029787, 0.1885948899914237, 0.26049034463034737, 0.1422874629497528, 0.050620796283086134, 0.05089438503438781, 0.16728743314743044, 0.06416246294975281, 0.012124331558451995, 0.024379203716913822, 0.20771253705024717, 0.2684268227645329, 0.24521253705024715, 0.0450652407275306, 0.07284298870298603, 0.09541246294975281, 0.03033158466929492, 0.04853746294975281, 0.07085889152118136, 0.08985542399542668, 0.07978746294975281, 0.017287462949752808, 0.18509348943119946, 0.10035959587377663, 0.07094786097021666, 0.10939272610764755, 0.0498961586019267, 0.06083756685256958, 0.12977139038198138, 0.21728746294975282, 0.1202125370502472, 0.01049034463034737, 0.017287462949752808, 0.1160458703835805, 0.07284301850530842, 0.017287462949752808, 0.01842685256685528, 0.08271256685256956, 0.08395412961641949, 0.03291246294975281, 0.08271253705024717, 0.09382364816135841, 0.0699190418971212, 0.12022863942034107, 0.19937920371691387, 0.06140511000857632, 0.004129568212910717, 0.029587537050247192, 0.14937923351923615, 0.017287462949752808, 0.4827125370502472, 0.02021253705024717, 0.0005696799073900971, 0.10062079628308612, 0.08271256685256956, 0.031176322036319315, 0.34382364816135835, 0.1545875370502472, 0.017287462949752808, 0.0304453576865949, 0.08871603437832426, 0.03826809260580277, 0.03395409981409711, 0.0006207962830860891, 0.08395412961641949, 0.10062079628308612, 0.06083756685256958, 0.10657317723546711, 0.0016624629497528076, 0.16453071886842896, 0.09228746294975282, 0.21604587038358047, 0.3755697097097125, 0.0719982513359615, 0.09623483137080546, 0.017287462949752808, 0.3420875370502472, 0.04521256685256958, 0.08673187759187481, 0.11103746294975281, 0.1422874629497528, 0.07094783116789427, 0.07978746294975281, 0.2493792037169139, 0.14937920371691377, 0.010490314828024982, 0.057712537050247203, 0.07094783116789427, 0.09382364816135841, 0.1827125370502472, 0.15918312528554135, 0.12333753705024719, 0.050620796283086134, 0.14937920371691377, 0.16728746294975283, 0.005923826586116454, 0.2795875370502472, 0.08395412961641949, 0.07978746294975281, 0.06604587038358056, 0.007712537050247215, 0.0672874629497528, 0.03271253705024724, 0.07085886171885897, 0.2505696799073901, 0.03514460580689566, 0.3279506620906648, 0.017287462949752808, 0.012124301756129607, 0.0025815805968116834, 0.09382364816135841, 0.1389625370502472, 0.08395412961641949, 0.031176351838641703, 0.07094783116789427, 0.031176351838641703, 0.11728746294975279, 0.07978746294975281, 0.2327125370502472, 0.11103746294975281, 0.3577125370502472, 0.04521256685256958, 0.09634893048893328, 0.2327125370502472, 0.1702125370502472, 0.08395412961641949, 0.06416246294975281, 0.1969982513359615, 0.041096956956954284, 0.16014460580689566, 0.05300171886171612, 0.04853746294975281, 0.20062079628308616, 0.219554642313405, 0.04506521092520821, 0.0867319073941972, 0.03291246294975281, 0.32094783116789427, 0.12022860961801868, 0.10062079628308612, 0.10657317723546711, 0.13271253705024721, 0.02021253705024717, 0.053001748664038506, 0.07085889152118136, 0.27117407551178563, 0.013962537050247192, 0.04521253705024719, 0.04669922765563517, 0.08395412961641949, 0.1389625370502472, 0.0699190418971212, 0.09634893048893328, 0.02958756685256958, 0.017287462949752808, 0.029587537050247192, 0.04669922765563517, 0.056761147160279135, 0.07646256685256958, 0.07410564476793466, 0.0672874629497528, 0.10035959587377663, 0.21708756685256958, 0.03271253705024724, 0.08985539419310429, 0.05478746294975281, 0.05963561397332412, 0.31604587038358056, 0.11506547822671775, 0.2660458703835805, 0.18583756685256958, 0.009028326523931374, 0.08271253705024717, 0.08053862400676887, 0.10771253705024719, 0.03271253705024724, 0.21371603437832426, 0.06083756685256958, 0.16434628647916455, 0.0867319073941972, 0.07886638320409328, 0.09683291749520734, 0.1240169146786565, 0.06604587038358056, 0.10552275706739989, 0.07497977064206052, 0.03826809260580277, 0.04521256685256958, 0.055749001411291216, 0.10771253705024719, 0.0699190418971212, 0.0649065105688004, 0.017287462949752808, 0.1389625370502472, 0.0672874629497528, 0.26049034463034737, 0.1922874629497528, 0.10112575167103821, 0.10771253705024719, 0.07085889152118136, 0.04853746294975281, 0.3577125370502472, 0.10113358968182617, 0.017287462949752808, 0.11728746294975279, 0.04506521092520821, 0.017287462949752808, 0.010490314828024982, 0.14521253705024717, 0.01604587038358052, 0.056241948814953036, 0.06083756685256958, 0.1780017486640385, 0.01604587038358052, 0.16326809260580277, 0.03271253705024724, 0.0699190418971212, 0.03826809260580277, 0.11450968517197502, 0.11450968517197502, 0.11728746294975279, 0.0672874629497528, 0.050620796283086134, 0.04521253705024719, 0.049379203716913844, 0.029587537050247192, 0.04506521092520821, 0.05002022935793948, 0.1266624629497528, 0.01396256685256958, 0.035344115997615666, 0.10657314743314472, 0.11103746294975281, 0.0485020107344577, 0.03953071886842896, 0.19937923351923625, 0.1885949197937461, 0.09937920371691389, 0.1561763518386417, 0.24833756685256958, 0.3577125370502472, 0.05215698149469161, 0.03199331550037163, 0.1160458703835805, 0.049379203716913844, 0.02218622126077352, 0.017287462949752808, 0.031993345302694015, 0.0304453576865949, 0.0450652407275306, 0.053001748664038506, 0.10657317723546711, 0.03291246294975281, 0.08565374332315778, 0.0035458703835805627, 0.08395412961641949, 0.04521253705024719, 0.05624197861727542, 0.04853746294975281, 0.02978746294975282, 0.08985539419310429, 0.01049034463034737, 0.024379203716913822, 0.017287462949752808, 0.0339541296164195, 0.01728743314743042, 0.01728743314743042, 0.31604587038358056, 0.017287462949752808, 0.036518232180522, 0.03271253705024724, 0.0016624629497528076, 0.2781670825047926, 0.01728743314743042, 0.05215698149469161, 0.0016624629497528076, 0.056242097826564974, 0.01728743314743042, 0.0422874629497528, 0.017287462949752808, 0.07886638320409328, 0.01728743314743042, 0.0053826712426685575, 0.10771253705024719, 0.15918312528554135, 0.10035959587377663, 0.4827125370502472, 0.09521253705024718, 0.03033158466929492, 0.2577125668525696, 0.2188236481613584, 0.2327125370502472, 0.017287462949752808, 0.0006207664807637014, 0.03291246294975281, 0.07284301850530842, 0.017287462949752808, 0.1283985740608639, 0.02978746294975282, 0.02218622126077352, 0.07978746294975281, 0.09208756685256958, 0.0005696799073900971, 0.035344115997615666, 0.2327125370502472, 0.21800668449962846, 0.017287462949752808, 0.06416243314743042, 0.050620796283086134, 0.08395412961641949, 0.07021253705024721, 0.049379203716913844, 0.050620796283086134, 0.017287462949752808, 0.2014625370502472, 0.1561763518386417, 0.1545875370502472, 0.08271253705024717, 0.08395412961641949, 0.13271253705024721, 0.02978746294975282, 0.1827125370502472, 0.017287462949752808, 0.04669922765563517, 0.1172874331474304, 0.03514460580689566, 0.10657314743314472, 0.013962537050247192, 0.2483375370502472, 0.07085889152118136, 0.01604587038358052, 0.050620766480763746, 0.04223634657405673, 0.029192224854514698, 0.2862839656216758, 0.45771253705024717, 0.2870603929395261, 0.14447724293260011, 0.05215698149469161, 0.017287462949752808, 0.1612839656216758, 0.129771360579659, 0.03291246294975281, 0.08307693663396332, 0.10478746294975283, 0.1422874629497528, 0.03826809260580277, 0.09208753705024719, 0.017287462949752808, 0.017287462949752808, 0.03826809260580277, 0.0908168747144587, 0.06966905878937768, 0.07085889152118136, 0.047929928354594975, 0.06140511000857632, 0.0404048447425549, 0.056761147160279135, 0.1389625370502472, 0.0025815805968116834, 0.11812920371691377, 0.01604587038358052, 0.11103746294975281, 0.14964040412622337, 0.09382364816135841, 0.01728743314743042, 0.17388900763848247, 0.1885948899914237, 0.07646253705024719, 0.10035959587377663, 0.08985539419310429, 0.0699190418971212, 0.017287462949752808, 0.09228746294975282, 0.20062079628308616, 0.0025815805968116834, 0.017287462949752808, 0.012124331558451995, 0.16728746294975283, 0.0649065105688004, 0.2049347592724694, 0.23950968517197502, 0.017287462949752808, 0.0006207962830860891, 0.14964040412622337, 0.2639625370502472, 0.0485020107344577, 0.07978746294975281, 0.24741841940318832, 0.4549347592724694, 0.07978746294975281, 0.0672874629497528, 0.2327125370502472, 0.08307693663396332, 0.1969982513359615, 0.03291246294975281, 0.09623483137080546, 0.2660459001859029, 0.14447724293260011, 0.0016624629497528076, 0.017287462949752808, 0.0450652407275306, 0.0799347592724694, 0.02437923351923621, 0.006522090662093416, 0.031993345302694015, 0.06991901209479881, 0.30771253705024715, 0.1839541296164195, 0.33271253705024717, 0.09382364816135841, 0.07094783116789427, 0.012124301756129607, 0.007712537050247215, 0.0799347592724694, 0.10062079628308612, 0.04521253705024719, 0.2327125370502472, 0.0415360962643343, 0.08871603437832426, 0.0963489006866109, 0.13493452177328225, 0.21728746294975282, 0.017287462949752808, 0.031176351838641703, 0.11429148441866821, 0.012124301756129607, 0.01049034463034737, 0.004129568212910717, 0.15062079628308617, 0.3248178002081419, 0.24660142593913603, 0.21800665469730607, 0.032712566852569624, 0.042287433147430414, 0.04669922765563517, 0.16312079628308612, 0.19937920371691387, 0.07795063228834243, 0.04669922765563517, 0.0016624629497528076, 0.0339541296164195, 0.07646253705024719, 0.1422874629497528, 0.02218622126077352, 0.0304453576865949, 0.04153606646201191, 0.036518232180522, 0.0450652407275306, 0.08871603437832426, 0.08498526432297449, 0.02021253705024717, 0.026830184109070676, 0.017287462949752808, 0.2672874331474304, 0.07094783116789427, 0.1422874629497528, 0.031176351838641703, 0.14937920371691377, 0.11103746294975281, 0.0005697097097124848, 0.2218429718328559, 0.0025815805968116834, 0.1422874629497528, 0.09541246294975281, 0.21728746294975282, 0.03291246294975281, 0.04669922765563517, 0.03826809260580277, 0.07362162795933808, 0.03826809260580277, 0.06604587038358056, 0.0839540998140971, 0.10771256685256958, 0.1827125370502472, 0.4827125370502472, 0.07611099236151753, 0.029587537050247192, 0.017287462949752808, 0.0339541296164195, 0.40918312528554135, 0.16128399542399818, 0.21604587038358047, 0.05215698149469161, 0.11728746294975279, 0.07094783116789427, 0.0304453576865949, 0.0672874629497528, 0.05895412961641952, 0.2049347592724694, 0.04937923351923623, 0.1839541296164195, 0.028167082504792607, 0.0672874629497528, 0.260490314828025, 0.2639625370502472, 0.30771253705024715, 0.04853746294975281, 0.1827125370502472, 0.08271253705024717, 0.06140511000857632, 0.0304453576865949, 0.11506547822671775, 0.031176351838641703, 0.24587046158941173, 0.07094783116789427, 0.16728746294975283, 0.06604587038358056, 0.031176351838641703, 0.1327125668525696, 0.11506550802904014, 0.031176351838641703, 0.1922874629497528, 0.07094783116789427, 0.007712537050247215, 0.028167082504792607, 0.1791411084788186, 0.1827125370502472, 0.04153606646201191, 0.05215698149469161, 0.0908168747144587, 0.1422874629497528, 0.05215698149469161, 0.056761147160279135, 0.06416246294975281, 0.003398574060863968, 0.049379203716913844, 0.08271253705024717, 0.0672874629497528, 0.017287462949752808, 0.010490314828024982, 0.031176351838641703, 0.03291246294975281, 0.09228746294975282, 0.08871603437832426, 0.3041411084788186, 0.2672874629497528, 0.03514460580689566, 0.004129568212910717, 0.09541246294975281, 0.07094783116789427, 0.0005697097097124848, 0.0450652407275306, 0.0856537135208354, 0.036283965621675796, 0.11103746294975281, 0.2577125668525696, 0.09208753705024719, 0.01842682276453289, 0.4358375370502472, 0.005923826586116454, 0.1389625370502472, 0.01604587038358052, 0.012124301756129607, 0.06140511000857632, 0.010490314828024982, 0.01604587038358052, 0.21728746294975282, 0.06604587038358056, 0.29937920371691384, 0.07978746294975281, 0.2672874629497528, 0.2170875370502472, 0.032712566852569624, 0.04853746294975281, 0.2014625370502472, 0.0963489006866109, 0.09382364816135841, 0.03479587038358056, 0.09228746294975282, 0.1422874629497528, 0.05895412961641952, 0.24587043178708934, 0.03271253705024724, 0.017287462949752808, 0.0450652407275306, 0.2327125370502472, 0.02865109931338916, 0.0005696799073900971, 0.04853743314743042, 0.09937920371691389, 0.05414110847881859, 0.06604587038358056, 0.049379203716913844, 0.053001748664038506, 0.031993345302694015, 0.07978746294975281, 0.031176351838641703, 0.12443032009260996, 0.01842682276453289, 0.1406072738923525, 0.2684268227645329, 0.04521253705024719, 0.15190284756513744, 0.15918312528554135, 0.06271253705024721, 0.042287433147430414, 0.1422874629497528, 0.057712537050247203, 0.2001038413980733, 0.07978743314743042, 0.2639625370502472, 0.029587537050247192, 0.06140511000857632, 0.11450965536965263, 0.056241948814953036, 0.05478746294975281, 0.09541246294975281, 0.4648553941931043, 0.08271253705024717, 0.2952125370502472, 0.10771256685256958, 0.06416246294975281, 0.08271253705024717, 0.08871603437832426, 0.12443032009260996, 0.050620766480763746, 0.12022863942034107, 0.04853746294975281, 0.1406073036946749, 0.005382701044990945, 0.050620796283086134, 0.026830184109070676, 0.007712537050247215, 0.05002022935793948, 0.0672874629497528, 0.08985539419310429, 0.007712537050247215, 0.0450652407275306, 0.005382701044990945, 0.0702125668525696, 0.09541246294975281, 0.04669922765563517, 0.036283965621675796, 0.050620766480763746, 0.1283985740608639, 0.0799347592724694, 0.06604590018590295, 0.10035959587377663, 0.2483375370502472, 0.003398574060863968, 0.036283965621675796, 0.035344145799938054, 0.013962537050247192, 0.0006207962830860891, 0.16728746294975283, 0.006522060859771028, 0.07611099236151753, 0.01728743314743042, 0.04669919785331278, 0.14447724293260011, 0.017287462949752808, 0.050620796283086134, 0.04521253705024719, 0.0005696799073900971, 0.12443032009260996, 0.21728746294975282, 0.03514460580689566, 0.0025815507944892957, 0.09228746294975282, 0.013962537050247192, 0.0485020107344577, 0.129771360579659, 0.03291246294975281, 0.06083753705024719, 0.09937920371691389, 0.028167082504792607, 0.0699190418971212, 0.05414113828114098, 0.07978746294975281, 0.0006207962830860891, 0.1266624629497528, 0.1389625370502472, 0.1922874629497528, 0.0856537135208354, 0.07085889152118136, 0.37021253705024715, 0.06604587038358056, 0.1422874629497528, 0.12333753705024719, 0.01604587038358052, 0.04853746294975281, 0.10771253705024719, 0.0304453576865949, 0.012124301756129607, 0.0047874629497527965, 0.08871600457600187, 0.12022863942034107, 0.03271253705024724, 0.1612839656216758, 0.10062079628308612, 0.02021253705024717, 0.10062076648076373, 0.017287462949752808, 0.017287462949752808, 0.2672874629497528, 0.06140508020625393, 0.04669922765563517, 0.04669922765563517, 0.049379203716913844, 0.04153606646201191, 0.0450652407275306, 0.08565374332315778, 0.03826809260580277, 0.10771253705024719, 0.302157011297014, 0.04853746294975281, 0.0799347592724694, 0.0016624629497528076, 0.003398574060863968, 0.0054398097775198995, 0.07978743314743042, 0.16014460580689566, 0.10062079628308612, 0.029192224854514698, 0.0485020107344577, 0.08271253705024717, 0.0339541296164195, 0.01842682276453289, 0.07646253705024719, 0.043603252423437044, 0.07611099236151753, 0.04153606646201191, 0.06225799159570178, 0.03291246294975281, 0.04853746294975281, 0.08871603437832426, 0.0672874629497528, 0.055749001411291216, 0.049379203716913844, 0.03514460580689566, 0.017287462949752808, 0.056241948814953036, 0.017287462949752808, 0.017287462949752808, 0.06416246294975281, 0.01728743314743042, 0.09937920371691389, 0.10657317723546711, 0.2049347592724694, 0.017287462949752808, 0.07795063228834243, 0.04669922765563517, 0.1702125370502472, 0.1545875370502472, 0.27117407551178563, 0.04153606646201191, 0.06140508020625393, 0.053001748664038506, 0.057712537050247203, 0.10771253705024719, 0.07978746294975281, 0.06165990547129985, 0.031176351838641703, 0.0005696799073900971, 0.031176351838641703, 0.2915360962643343, 0.024379203716913822, 0.2660458703835805, 0.03826809260580277, 0.053001748664038506, 0.12333753705024719, 0.07481780020814188, 0.01049034463034737, 0.01728743314743042, 0.06165990547129985, 0.017287462949752808, 0.07978746294975281, 0.10552275706739989, 0.017287462949752808, 0.043603252423437044, 0.206396747576563, 0.024379203716913822, 0.06416243314743042, 0.1422874629497528, 0.05300171886171612, 0.029587537050247192, 0.11103746294975281, 0.1791411084788186, 0.0304453576865949, 0.009028326523931374, 0.18583756685256958, 0.08307693663396332, 0.14228743314743042, 0.0867319073941972, 0.05414110847881859, 0.2768301841090707, 0.16604587038358054, 0.1969982513359615, 0.0908168747144587, 0.15771253705024718, 0.09937920371691389, 0.30624194881495304, 0.2327125370502472, 0.05414110847881859, 0.16326809260580277, 0.050620796283086134, 0.04669922765563517, 0.0006207664807637014, 0.03514460580689566, 0.10552275706739989, 0.07646253705024719, 0.16604587038358054, 0.05215698149469161, 0.06604587038358056, 0.03271253705024724, 0.0025815507944892957]\n",
            "Test loss: 0.096\n",
            "Test accuracy: 36.314%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anfDvf0K_Te6",
        "outputId": "704ba538-b411-45e7-9318-6d2bd3f022dc"
      },
      "source": [
        "print(\"END\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "END\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}